{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization - Frequency based algorithm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojith18/Text_Summarization/blob/main/Text_Summarization_Frequency_based_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUEq8nGVXtwL"
      },
      "source": [
        "# Text summarization - Frequency based algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCiAiO8LULAm"
      },
      "source": [
        "# Preprocessing the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HZjp-jubESI"
      },
      "source": [
        "import re # relugar expression\n",
        "import nltk # natural language toolkit\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BesYuheBbWp8"
      },
      "source": [
        "# I added the word machine at the end of the last sentence\n",
        "original_text = \"\"\"Artificial intelligence is human like intelligence. \n",
        "                   It is the study of intelligent artificial agents. \n",
        "                   Science and engineering to produce intelligent machines. \n",
        "                   Solve problems and have intelligence. \n",
        "                   Related to intelligent behavior. \n",
        "                   Developing of reasoning machines. \n",
        "                   Learn from mistakes and successes. \n",
        "                   Artificial intelligence is related to reasoning in everyday situations.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "dm1fqjpobw11",
        "outputId": "da97d468-0212-4438-b1d9-a48799b8f2eb"
      },
      "source": [
        "original_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence. \\n                   It is the study of intelligent artificial agents. \\n                   Science and engineering to produce intelligent machines. \\n                   Solve problems and have intelligence. \\n                   Related to intelligent behavior. \\n                   Developing of reasoning machines. \\n                   Learn from mistakes and successes. \\n                   Artificial intelligence is related to reasoning in everyday situations.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CSuPnw3b-9u"
      },
      "source": [
        "original_text = re.sub(r'\\s+', ' ', original_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "TNiNkUCwccI6",
        "outputId": "d1663b36-01de-4da8-a887-de8cf476e47e"
      },
      "source": [
        "original_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFEnOfcIehoU",
        "outputId": "a90b019d-2073-4cbc-ec67-ebdcc81f8236"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEUpuAWWfBIo",
        "outputId": "7dd35a13-0585-4e7a-8985-f173bc14a10e"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSsuVgB4fG2m",
        "outputId": "3145b307-bb42-44a9-c0fd-292e0ab4f1ad"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdd8VPuwfcgG",
        "outputId": "a884cfa9-5765-4fae-e0ae-4801e4feca30"
      },
      "source": [
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "wpJ2mHUmganw",
        "outputId": "769bef34-4f5a-4a03-a7b1-de02aeb7480f"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR32GNDedjuL"
      },
      "source": [
        "def preprocess(text):\n",
        "  formatted_text = text.lower()\n",
        "  tokens = []\n",
        "  for token in nltk.word_tokenize(formatted_text):\n",
        "    tokens.append(token)\n",
        "  #print(tokens)\n",
        "  tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
        "  formatted_text = ' '.join(element for element in tokens)\n",
        "\n",
        "  return formatted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "icNQm0wLdxAK",
        "outputId": "06bef34c-26ea-4a87-ad54-6ddacc0d52c1"
      },
      "source": [
        "formatted_text = preprocess(original_text)\n",
        "formatted_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'artificial intelligence human like intelligence study intelligent artificial agents science engineering produce intelligent machines solve problems intelligence related intelligent behavior developing reasoning machines learn mistakes successes artificial intelligence related reasoning everyday situations'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9qp7RfhZAaK"
      },
      "source": [
        "# Word frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjOMn0eUiT8d",
        "outputId": "4e6ec08f-c064-409b-e75e-3b0e060b54bd"
      },
      "source": [
        "word_frequency = nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
        "word_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'agents': 1,\n",
              "          'artificial': 3,\n",
              "          'behavior': 1,\n",
              "          'developing': 1,\n",
              "          'engineering': 1,\n",
              "          'everyday': 1,\n",
              "          'human': 1,\n",
              "          'intelligence': 4,\n",
              "          'intelligent': 3,\n",
              "          'learn': 1,\n",
              "          'like': 1,\n",
              "          'machines': 2,\n",
              "          'mistakes': 1,\n",
              "          'problems': 1,\n",
              "          'produce': 1,\n",
              "          'reasoning': 2,\n",
              "          'related': 2,\n",
              "          'science': 1,\n",
              "          'situations': 1,\n",
              "          'solve': 1,\n",
              "          'study': 1,\n",
              "          'successes': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjXSn7tFjcsR",
        "outputId": "5db58dc4-7926-4a57-e6b8-75377163e375"
      },
      "source": [
        "word_frequency['intelligence']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYmClmlyjnSf",
        "outputId": "199e6b56-afff-4114-9750-ec9818c68f1b"
      },
      "source": [
        "word_frequency.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['artificial', 'intelligence', 'human', 'like', 'study', 'intelligent', 'agents', 'science', 'engineering', 'produce', 'machines', 'solve', 'problems', 'related', 'behavior', 'developing', 'reasoning', 'learn', 'mistakes', 'successes', 'everyday', 'situations'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdFljvgVjsSD",
        "outputId": "82e914e1-c51b-48e6-9fa6-5f72ae866017"
      },
      "source": [
        "len(word_frequency.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONWhD4F0lmvc",
        "outputId": "090ef05d-41f5-4d27-dc1c-b71538542573"
      },
      "source": [
        "highest_frequency = max(word_frequency.values())\n",
        "highest_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HD8rDzBmEo-"
      },
      "source": [
        "for word in word_frequency.keys():\n",
        "  #print(word)\n",
        "  word_frequency[word] = (word_frequency[word] / highest_frequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1tBc-8WmX0X",
        "outputId": "9a4ffe07-04bc-496c-d0db-a87150186898"
      },
      "source": [
        "word_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'agents': 0.25,\n",
              "          'artificial': 0.75,\n",
              "          'behavior': 0.25,\n",
              "          'developing': 0.25,\n",
              "          'engineering': 0.25,\n",
              "          'everyday': 0.25,\n",
              "          'human': 0.25,\n",
              "          'intelligence': 1.0,\n",
              "          'intelligent': 0.75,\n",
              "          'learn': 0.25,\n",
              "          'like': 0.25,\n",
              "          'machines': 0.5,\n",
              "          'mistakes': 0.25,\n",
              "          'problems': 0.25,\n",
              "          'produce': 0.25,\n",
              "          'reasoning': 0.5,\n",
              "          'related': 0.5,\n",
              "          'science': 0.25,\n",
              "          'situations': 0.25,\n",
              "          'solve': 0.25,\n",
              "          'study': 0.25,\n",
              "          'successes': 0.25})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SKF87yAZ3iW"
      },
      "source": [
        "# Sentence tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtjyIiSAoxZR",
        "outputId": "c4008e31-500d-47af-b8b5-401aff2c832c"
      },
      "source": [
        "'Phd John went home. He arrived early.'.split('.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Phd John went home', ' He arrived early', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpSYb6vapinA",
        "outputId": "5632d54c-9dae-4052-ba4c-7e6a4e420000"
      },
      "source": [
        "'Ph.d John went home. He arrived early.'.split('.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ph', 'd John went home', ' He arrived early', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyitMJq9pxJu",
        "outputId": "d7bbf1ed-b77d-4be1-86c5-cfbab0c4bd29"
      },
      "source": [
        "nltk.sent_tokenize('Ph.d John went home. He arrived early.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ph.d John went home.', 'He arrived early.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNrwIZJtqLZn",
        "outputId": "7d141e1b-c258-4342-db2b-cc4c2c9e49d0"
      },
      "source": [
        "sentence_list = nltk.sent_tokenize(original_text)\n",
        "sentence_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial intelligence is human like intelligence.',\n",
              " 'It is the study of intelligent artificial agents.',\n",
              " 'Science and engineering to produce intelligent machines.',\n",
              " 'Solve problems and have intelligence.',\n",
              " 'Related to intelligent behavior.',\n",
              " 'Developing of reasoning machines.',\n",
              " 'Learn from mistakes and successes.',\n",
              " 'Artificial intelligence is related to reasoning in everyday situations.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zNS8EciaQuC"
      },
      "source": [
        "# Generate the summary (score for sentences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwoSZAqbuFHg",
        "outputId": "d81c5ebb-5814-4c3b-a138-d4ad705bc100"
      },
      "source": [
        "word_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'agents': 0.25,\n",
              "          'artificial': 0.75,\n",
              "          'behavior': 0.25,\n",
              "          'developing': 0.25,\n",
              "          'engineering': 0.25,\n",
              "          'everyday': 0.25,\n",
              "          'human': 0.25,\n",
              "          'intelligence': 1.0,\n",
              "          'intelligent': 0.75,\n",
              "          'learn': 0.25,\n",
              "          'like': 0.25,\n",
              "          'machines': 0.5,\n",
              "          'mistakes': 0.25,\n",
              "          'problems': 0.25,\n",
              "          'produce': 0.25,\n",
              "          'reasoning': 0.5,\n",
              "          'related': 0.5,\n",
              "          'science': 0.25,\n",
              "          'situations': 0.25,\n",
              "          'solve': 0.25,\n",
              "          'study': 0.25,\n",
              "          'successes': 0.25})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHQkxxxNuOck"
      },
      "source": [
        "score_sentences = {}\n",
        "for sentence in sentence_list:\n",
        "  #print(sentence)\n",
        "  for word in nltk.word_tokenize(sentence.lower()):\n",
        "    #print(word)\n",
        "    if sentence not in score_sentences.keys():\n",
        "      score_sentences[sentence] = word_frequency[word]\n",
        "    else:\n",
        "      score_sentences[sentence] += word_frequency[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZFUrwYKv-jf",
        "outputId": "3eaa1b51-957b-4e57-ff54-0d7a766bc01b"
      },
      "source": [
        "score_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Artificial intelligence is human like intelligence.': 3.25,\n",
              " 'Artificial intelligence is related to reasoning in everyday situations.': 3.25,\n",
              " 'Developing of reasoning machines.': 1.25,\n",
              " 'It is the study of intelligent artificial agents.': 2.0,\n",
              " 'Learn from mistakes and successes.': 0.75,\n",
              " 'Related to intelligent behavior.': 1.5,\n",
              " 'Science and engineering to produce intelligent machines.': 2.0,\n",
              " 'Solve problems and have intelligence.': 1.5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WimqIslVwF6-",
        "outputId": "5d12204f-cb79-4d29-9ebf-a75b53c89205"
      },
      "source": [
        "score_sentences['Solve problems and have intelligence.']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of5mNwB4wLdA",
        "outputId": "80311597-2d3f-4519-fffc-61ca73e1917e"
      },
      "source": [
        "score_sentences.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Artificial intelligence is human like intelligence.', 'It is the study of intelligent artificial agents.', 'Science and engineering to produce intelligent machines.', 'Solve problems and have intelligence.', 'Related to intelligent behavior.', 'Developing of reasoning machines.', 'Learn from mistakes and successes.', 'Artificial intelligence is related to reasoning in everyday situations.'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XCIP6eGwP8B"
      },
      "source": [
        "import heapq\n",
        "best_sentences = heapq.nlargest(3, score_sentences, key = score_sentences.get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrapW2JPxc-I",
        "outputId": "d5755a0a-962f-4f1a-d0cf-6f4c59b384c9"
      },
      "source": [
        "best_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Artificial intelligence is human like intelligence.',\n",
              " 'Artificial intelligence is related to reasoning in everyday situations.',\n",
              " 'It is the study of intelligent artificial agents.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "X6EJOkU0xvhL",
        "outputId": "39143da9-830e-4503-dd3f-fa16df69f4b0"
      },
      "source": [
        "summary = ' '.join(best_sentences)\n",
        "summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence. Artificial intelligence is related to reasoning in everyday situations. It is the study of intelligent artificial agents.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "5ETE6w4ix5zQ",
        "outputId": "421d415d-0ff5-4149-8bfd-a5ea327b2835"
      },
      "source": [
        "original_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Artificial intelligence is human like intelligence. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C05hd_M5c4yr"
      },
      "source": [
        "# Visualizing the summary in HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IyA9d1GzTcK"
      },
      "source": [
        "from IPython.core.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "id": "_UGuvRA5zcDQ",
        "outputId": "0c755a6d-24eb-4d18-9d16-7d86a67d474b"
      },
      "source": [
        "text = ''\n",
        "display(HTML(f'<h2>Summary</h2>'))\n",
        "for sentence in sentence_list:\n",
        "  #print(sentence)\n",
        "  #text += sentence\n",
        "  if sentence in best_sentences:\n",
        "    text += ' ' + sentence.replace(sentence, f\"<mark>{sentence}</mark>\")\n",
        "  else:\n",
        "    text += ' ' + sentence\n",
        "\n",
        "display(HTML(f\"\"\"{text}\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>Summary</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              " <mark>Artificial intelligence is human like intelligence.</mark> <mark>It is the study of intelligent artificial agents.</mark> Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. <mark>Artificial intelligence is related to reasoning in everyday situations.</mark>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ0aG69oUxc2"
      },
      "source": [
        "# Extracting texts from the Internet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCYjnhxe2rm1"
      },
      "source": [
        "!pip install goose3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu4BYqTs2zb2"
      },
      "source": [
        "from goose3 import Goose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtTD4NJ3AgX"
      },
      "source": [
        "g = Goose()\n",
        "url = 'https://en.wikipedia.org/wiki/Automatic_summarization'\n",
        "article = g.extract(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37-3RNVG3YDh",
        "outputId": "e2334e76-9c1b-4279-8f88-4ef367d6c3e7"
      },
      "source": [
        "article.infos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'authors': [],\n",
              " 'cleaned_text': 'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\\n\\nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.[2]\\n\\nThere are two general approaches to automatic summarization: extraction and abstraction.\\n\\nHere, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[4]\\n\\nThis has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\\n\\nApproaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\\n\\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\n\\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\n\\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\\n\\nAt a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\\n\\nThe task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.[6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below. Consider the example text from a news article:\\n\\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system. Keyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\\n\\nDepending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.\\n\\nBeginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem. Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall. Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\\n\\nDesigning a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\\n\\nWe also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.\\n\\nIn the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\\n\\nOnce examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney\\'s GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\\n\\nAnother keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney\\'s results demonstrate. Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text\\'s intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\\n\\nTextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\\n\\nThe vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\\n\\nEdges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\\n\\nSince this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\\n\\nIt is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\\n\\nIn short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\\n\\nLike keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.\\n\\nBefore getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated. The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.\\n\\nIf there are multiple references, the ROUGE-1 scores are averaged. Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree. Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.\\n\\nA promising line in document summarization is adaptive document/text summarization.[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre. First summarizes that perform adaptive summarization have been created.[10]\\n\\nSupervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.\\n\\nDuring the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\\n\\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\\n\\nA more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\\n\\nIn both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\\n\\nThe edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences\\' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.\\n\\nIn both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\\n\\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\\n\\nAnother important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. The task remains the same in both cases—only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary. The method used is called Cross-Sentence Information Subsumption (CSIS).\\n\\nThese methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\\n\\nMulti-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.[12] [4]\\n\\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.[dubious – discuss]\\n\\nMulti-document extractive summarization faces a problem of potential redundancy. Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results. There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks. (An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.) The algorithm is called GRASSHOPPER.[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\\n\\nThe state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[16]\\n\\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed. This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\\n\\nThe idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\\n\\nWhile submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee.[17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\\n\\nSubmodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.[citation needed]\\n\\nSubmodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.[22]\\n• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.[24] The name is reference to TL;DR − Internet slang for \"too long; didn\\'t read\".[25][26]\\n\\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\\n\\nEvaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual.[28]\\n\\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\\n\\nIntra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.\\n\\nHuman judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.\\n\\nOne of the metrics used in NIST\\'s annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]). It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary\\'s coherence. Anaphor resolution remains another problem yet to be fully solved. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.[29]\\n\\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.[30]\\n\\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\\n\\nThe first publication in the area dates back to 1958 (Lun), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization.[31]\\n• None Roxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization.\\n• None Elena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation.\\n• None Alrehamy, Hassan (2017). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". Automatic Keyphrases Extraction. Advances in Intelligent Systems and Computing. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN .\\n• None Marcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. ISBN .\\n• , Published in Proceeding RIAO\\'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\\n• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks . , The GRASSHOPPER algorithm\\n• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). \"Summarizing Conceptual Graphs for Automatic Summarization Task\". Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN . , Conceptual Structures for STEM Research and Education.',\n",
              " 'domain': 'en.wikipedia.org',\n",
              " 'image': None,\n",
              " 'links': ['https://en.wikipedia.org/w/index.php?title=Automatic_summarization&action=edit',\n",
              "  '/wiki/Talk:Automatic_summarization',\n",
              "  '/wiki/Help:Maintenance_template_removal',\n",
              "  '/wiki/Wikipedia:Writing_better_articles#Tone',\n",
              "  '/wiki/Wikipedia:Writing_better_articles#Tone',\n",
              "  '/wiki/Help:Maintenance_template_removal',\n",
              "  '/wiki/Help:Maintenance_template_removal',\n",
              "  '/wiki/Abstract_(summary)',\n",
              "  '#cite_note-Torres2014-1',\n",
              "  '/wiki/Wikipedia:Citation_needed',\n",
              "  '#cite_note-PalPetrosino2012-2',\n",
              "  '#Approaches',\n",
              "  '#Extraction-based_summarization',\n",
              "  '#Abstraction-based_summarization',\n",
              "  '#Aided_summarization',\n",
              "  '#Applications_and_systems_for_summarization',\n",
              "  '#Keyphrase_extraction',\n",
              "  '#Supervised_learning_approaches',\n",
              "  '#Unsupervised_approach:_TextRank',\n",
              "  '#Document_summarization',\n",
              "  '#Supervised_learning_approaches_2',\n",
              "  '#Maximum_entropy-based_summarization',\n",
              "  '#TextRank_and_LexRank',\n",
              "  '#Multi-document_summarization',\n",
              "  '#Incorporating_diversity',\n",
              "  '#Submodular_functions_as_generic_tools_for_summarization',\n",
              "  '#Applications',\n",
              "  '#Evaluation_techniques',\n",
              "  '#Intrinsic_and_extrinsic_evaluation',\n",
              "  '#Inter-textual_and_intra-textual',\n",
              "  '#Domain_specific_versus_domain_independent_summarization_techniques',\n",
              "  '#Evaluating_summaries_qualitatively',\n",
              "  '#History',\n",
              "  '#See_also',\n",
              "  '#References',\n",
              "  '#Further_reading',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=1',\n",
              "  '/wiki/Information_extraction',\n",
              "  '/wiki/Abstract_(summary)',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=2',\n",
              "  '#cite_note-3',\n",
              "  '#cite_note-Afzal_et_al-4',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=3',\n",
              "  '/wiki/Automated_paraphrasing',\n",
              "  '/wiki/Natural_language_processing',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=4',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=5',\n",
              "  '/wiki/Cluster_analysis',\n",
              "  '/wiki/Multi-document_summarization',\n",
              "  '#cite_note-5',\n",
              "  '/wiki/Image_collection_exploration',\n",
              "  '/wiki/Submodular_set_function',\n",
              "  '/wiki/Determinantal_point_process',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=6',\n",
              "  '#cite_note-6',\n",
              "  '/wiki/Research_article',\n",
              "  '/wiki/Natural_language_understanding',\n",
              "  '/wiki/Information_retrieval',\n",
              "  '/wiki/Full-text_search',\n",
              "  '/wiki/Keyword_extraction',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=7',\n",
              "  '#cite_note-7',\n",
              "  '/wiki/Supervised_machine_learning',\n",
              "  '/wiki/Unigram',\n",
              "  '/wiki/Bigram',\n",
              "  '/wiki/Binary_classification',\n",
              "  '/wiki/Naive_Bayes',\n",
              "  '/wiki/Genetic_algorithm',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=8',\n",
              "  '/wiki/Training_set',\n",
              "  '#cite_note-8',\n",
              "  '/wiki/PageRank',\n",
              "  '/wiki/Social_network',\n",
              "  '/wiki/Graph_(abstract_data_type)',\n",
              "  '/wiki/Natural_language_processing',\n",
              "  '/wiki/Lexical_(semiotics)',\n",
              "  '/wiki/Semantic_similarity',\n",
              "  '/wiki/Eigenvalue',\n",
              "  '/wiki/Stationary_distribution',\n",
              "  '/wiki/Random_walk',\n",
              "  '/wiki/Co-occurrence',\n",
              "  '/wiki/Unigram',\n",
              "  '/wiki/Cohesion_(linguistics)',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=9',\n",
              "  '/wiki/ROUGE_(metric)',\n",
              "  '#cite_note-9',\n",
              "  '#cite_note-10',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=10',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=11',\n",
              "  '/wiki/Netherlands_Organisation_for_Applied_Scientific_Research',\n",
              "  '/wiki/Naive_Bayes',\n",
              "  '/wiki/Maximum_entropy_classifier',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=12',\n",
              "  '/wiki/Centroid',\n",
              "  '#cite_note-11',\n",
              "  '/wiki/Cosine_similarity',\n",
              "  '/wiki/TF-IDF',\n",
              "  '/wiki/Quantile_normalization',\n",
              "  '/wiki/Similarity_score',\n",
              "  '/w/index.php?title=MEAD&action=edit&redlink=1',\n",
              "  '/wiki/Linear_combination',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=13',\n",
              "  '/wiki/Multi-document_summarization',\n",
              "  '/wiki/News_aggregators',\n",
              "  '/wiki/Information_overload',\n",
              "  '#cite_note-12',\n",
              "  '#cite_note-Afzal_et_al-4',\n",
              "  '/wiki/Wikipedia:Accuracy_dispute#Disputed_statement',\n",
              "  '/wiki/Talk:Automatic_summarization#Dubious',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=14',\n",
              "  '#cite_note-13',\n",
              "  '/wiki/Absorbing_Markov_chain',\n",
              "  '#cite_note-14',\n",
              "  '#cite_note-15',\n",
              "  '#cite_note-16',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=15',\n",
              "  '/wiki/Submodular_set_function',\n",
              "  '/wiki/Combinatorial_optimization',\n",
              "  '/wiki/Set_cover_problem',\n",
              "  '/wiki/Facility_location_problem',\n",
              "  '/wiki/Determinantal_point_process',\n",
              "  '/wiki/Greedy_algorithm',\n",
              "  '#cite_note-17',\n",
              "  '#cite_note-18',\n",
              "  '#cite_note-19',\n",
              "  '/wiki/Wikipedia:Citation_needed',\n",
              "  '#cite_note-20',\n",
              "  '#cite_note-21',\n",
              "  '#cite_note-22',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=16',\n",
              "  '/wiki/File:Wiki_letter_w_cropped.svg',\n",
              "  'https://en.wikipedia.org/w/index.php?title=Automatic_summarization&action=edit&section=',\n",
              "  '/wiki/Reddit',\n",
              "  '/wiki/Internet_bot',\n",
              "  '#cite_note-23',\n",
              "  '#cite_note-24',\n",
              "  '/wiki/TL;DR',\n",
              "  '/wiki/Internet_slang',\n",
              "  '#cite_note-25',\n",
              "  '#cite_note-26',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=17',\n",
              "  '#cite_note-27',\n",
              "  '#cite_note-28',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=18',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=19',\n",
              "  '/wiki/Coherence_(linguistics)',\n",
              "  '/wiki/NIST',\n",
              "  'https://web.archive.org/web/20060408135021/http://haydn.isi.edu/ROUGE/',\n",
              "  '/wiki/N-gram',\n",
              "  '/wiki/Anaphora_(linguistics)',\n",
              "  '#cite_note-29',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=20',\n",
              "  '#cite_note-30',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=21',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=22',\n",
              "  '/wiki/Term_frequency%E2%80%93inverse_document_frequency',\n",
              "  '/wiki/Latent_semantic_analysis',\n",
              "  '/wiki/Non-negative_matrix_factorization',\n",
              "  '#cite_note-31',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=23',\n",
              "  '/wiki/Sentence_extraction',\n",
              "  '/wiki/Text_mining',\n",
              "  '/wiki/Multi-document_summarization',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=24',\n",
              "  '#cite_ref-Torres2014_1-0',\n",
              "  'https://www.wiley.com/en-gb/Automatic+Text+Summarization-p-9781848216686',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-1-848-21668-6',\n",
              "  '#cite_ref-PalPetrosino2012_2-0',\n",
              "  'https://books.google.com/books?id=O0fNBQAAQBAJ&q=video+surveillance+summarization&pg=PA81',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-1-4398-5685-7',\n",
              "  '#cite_ref-3',\n",
              "  'https://www.dummies.com/education/language-arts/speed-reading/how-to-skim-text/',\n",
              "  '#cite_ref-Afzal_et_al_4-0',\n",
              "  '#cite_ref-Afzal_et_al_4-1',\n",
              "  'https://www.jmir.org/2020/10/e19810/',\n",
              "  '#cite_ref-5',\n",
              "  '/wiki/Wikipedia:Templates_for_discussion/Log/2021_May_11#Template:Hide_in_print',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1007%2F978-3-642-10268-4_64',\n",
              "  '#cite_ref-6',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1007%2F978-3-319-66939-7_19',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-3-319-66938-0',\n",
              "  '#cite_ref-7',\n",
              "  '/wiki/ArXiv_(identifier)',\n",
              "  '//arxiv.org/abs/cs/0212020',\n",
              "  '/wiki/Bibcode_(identifier)',\n",
              "  'https://ui.adsabs.harvard.edu/abs/2002cs.......12020T',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1023%2FA%3A1009976227802',\n",
              "  '/wiki/S2CID_(identifier)',\n",
              "  'https://api.semanticscholar.org/CorpusID:7007323',\n",
              "  '#cite_ref-8',\n",
              "  'https://web.archive.org/web/20120617170501/http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf',\n",
              "  'http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf',\n",
              "  '#cite_ref-9',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.3103%2FS0005105510030027',\n",
              "  '/wiki/S2CID_(identifier)',\n",
              "  'https://api.semanticscholar.org/CorpusID:1586931',\n",
              "  '#cite_ref-10',\n",
              "  'http://yatsko.zohosites.com/universal-summarizer-unis.html',\n",
              "  '#cite_ref-11',\n",
              "  'https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html',\n",
              "  '#cite_ref-12',\n",
              "  'https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis',\n",
              "  '#cite_ref-13',\n",
              "  'https://www.cs.cmu.edu/afs/.cs.cmu.edu/Web/People/jgc/publication/MMR_DiversityBased_Reranking_SIGIR_1998.pdf',\n",
              "  '#cite_ref-14',\n",
              "  'http://www.aclweb.org/anthology/N07-1013',\n",
              "  '#cite_ref-15',\n",
              "  'https://arxiv.org/abs/1210.4871',\n",
              "  '#cite_ref-16',\n",
              "  'http://www.nowpublishers.com/article/DownloadSummary/MAL-044',\n",
              "  '#cite_ref-17',\n",
              "  '#cite_ref-18',\n",
              "  'https://arxiv.org/abs/1210.4871',\n",
              "  '#cite_ref-19',\n",
              "  'http://www.aclweb.org/anthology/P11-1052',\n",
              "  '#cite_ref-20',\n",
              "  'http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf',\n",
              "  '#cite_ref-21',\n",
              "  'http://www.aclweb.org/anthology/P15-1054',\n",
              "  '#cite_ref-22',\n",
              "  'http://www.jmlr.org/proceedings/papers/v37/wei15.pdf',\n",
              "  '#cite_ref-23',\n",
              "  'https://www.reddit.com/user/autotldr',\n",
              "  '#cite_ref-24',\n",
              "  '/wiki/Megan_Squire',\n",
              "  'https://books.google.com/books?id=_qXWDQAAQBAJ&pg=PA185',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/9781785885914',\n",
              "  '#cite_ref-25',\n",
              "  'https://www.lifewire.com/what-is-tldr-2483633',\n",
              "  '#cite_ref-26',\n",
              "  'http://www.ibtimes.com/what-does-tldr-mean-ama-til-glossary-reddit-terms-abbreviations-431704',\n",
              "  '#cite_ref-27',\n",
              "  'http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/sum-mani.pdf',\n",
              "  '#cite_ref-28',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.3103%2FS0005105507030041',\n",
              "  '/wiki/S2CID_(identifier)',\n",
              "  'https://api.semanticscholar.org/CorpusID:7853204',\n",
              "  '#cite_ref-29',\n",
              "  'http://papers.nips.cc/paper/5415-learning-mixtures-of-submodular-functions-for-image-collection-summarization.pdf',\n",
              "  '#cite_ref-30',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1007%2F978-3-642-38326-7_41',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-3-642-38325-0',\n",
              "  '#cite_ref-31',\n",
              "  'https://www.sciencedirect.com/science/article/pii/S1319157820303712',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1016%2Fj.jksuci.2020.05.006',\n",
              "  '/wiki/ISSN_(identifier)',\n",
              "  '//www.worldcat.org/issn/1319-1578',\n",
              "  '/w/index.php?title=Automatic_summarization&action=edit&section=25',\n",
              "  'https://www.researchgate.net/publication/277288103',\n",
              "  'https://www.researchgate.net/publication/2553088',\n",
              "  'https://www.cs.ru.nl/~kraaijw/pubs/Biblio/papers/meeting_sum_tno.pdf',\n",
              "  'https://repository.upenn.edu/cgi/viewcontent.cgi?article=1762&context=cis_papers',\n",
              "  'http://www.informatica.si/ojs-2.4.3/index.php/informatica/article/download/273/269',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1007%2F978-3-319-66939-7_19',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-3-319-66938-0',\n",
              "  'https://archive.org/details/springer_10.1007-978-3-642-72025-3',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-3-540-63735-6',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-0-262-13372-2',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-1-58811-060-2',\n",
              "  'http://www.jason-huff.com/projects/autosummarize/',\n",
              "  'http://portal.acm.org/citation.cfm?id=1937055.1937111&coll=DL&dl=GUIDE&CFID=23185814&CFTOKEN=40272014/',\n",
              "  'http://pages.cs.wisc.edu/~jerryzhu/pub/grasshopper.pdf',\n",
              "  '/wiki/Doi_(identifier)',\n",
              "  'https://doi.org/10.1007%2F978-3-642-35786-2_18',\n",
              "  '/wiki/ISBN_(identifier)',\n",
              "  '/wiki/Special:BookSources/978-3-642-35785-5',\n",
              "  '/wiki/Natural_language_processing',\n",
              "  '/wiki/AI-complete',\n",
              "  '/wiki/Bag-of-words_model',\n",
              "  '/wiki/N-gram',\n",
              "  '/wiki/Bigram',\n",
              "  '/wiki/Trigram',\n",
              "  '/wiki/Natural_language_understanding',\n",
              "  '/wiki/Speech_corpus',\n",
              "  '/wiki/Stop_words',\n",
              "  '/wiki/Text_corpus',\n",
              "  '/wiki/Text_mining',\n",
              "  '/wiki/Collocation_extraction',\n",
              "  '/wiki/Concept_mining',\n",
              "  '/wiki/Compound_term_processing',\n",
              "  '/wiki/Coreference#Coreference_resolution',\n",
              "  '/wiki/Lemmatisation',\n",
              "  '/wiki/Named-entity_recognition',\n",
              "  '/wiki/Ontology_learning',\n",
              "  '/wiki/Parsing',\n",
              "  '/wiki/Part-of-speech_tagging',\n",
              "  '/wiki/Semantic_similarity',\n",
              "  '/wiki/Sentiment_analysis',\n",
              "  '/wiki/Stemming',\n",
              "  '/wiki/Terminology_extraction',\n",
              "  '/wiki/Shallow_parsing',\n",
              "  '/wiki/Text_segmentation',\n",
              "  '/wiki/Sentence_boundary_disambiguation',\n",
              "  '/wiki/Word#Word_boundaries',\n",
              "  '/wiki/Textual_entailment',\n",
              "  '/wiki/Truecasing',\n",
              "  '/wiki/Word-sense_disambiguation',\n",
              "  '/wiki/Multi-document_summarization',\n",
              "  '/wiki/Sentence_extraction',\n",
              "  '/wiki/Text_simplification',\n",
              "  '/wiki/Machine_translation',\n",
              "  '/wiki/Computer-assisted_translation',\n",
              "  '/wiki/Example-based_machine_translation',\n",
              "  '/wiki/Rule-based_machine_translation',\n",
              "  '/wiki/Neural_machine_translation',\n",
              "  '/wiki/Automatic_identification_and_data_capture',\n",
              "  '/wiki/Speech_recognition',\n",
              "  '/wiki/Speech_segmentation',\n",
              "  '/wiki/Speech_synthesis',\n",
              "  '/wiki/Natural_language_generation',\n",
              "  '/wiki/Optical_character_recognition',\n",
              "  '/wiki/Topic_model',\n",
              "  '/wiki/Latent_Dirichlet_allocation',\n",
              "  '/wiki/Latent_semantic_analysis',\n",
              "  '/wiki/Pachinko_allocation',\n",
              "  '/wiki/Computer-assisted_reviewing',\n",
              "  '/wiki/Automated_essay_scoring',\n",
              "  '/wiki/Concordancer',\n",
              "  '/wiki/Grammar_checker',\n",
              "  '/wiki/Predictive_text',\n",
              "  '/wiki/Spell_checker',\n",
              "  '/wiki/Syntax_guessing',\n",
              "  '/wiki/Natural_language_user_interface',\n",
              "  '/wiki/Chatbot',\n",
              "  '/wiki/Interactive_fiction',\n",
              "  '/wiki/Question_answering',\n",
              "  '/wiki/Virtual_assistant',\n",
              "  '/wiki/Voice_user_interface'],\n",
              " 'meta': {'canonical': 'https://en.wikipedia.org/wiki/Automatic_summarization',\n",
              "  'description': '',\n",
              "  'encoding': 'UTF-8',\n",
              "  'favicon': '/static/apple-touch/wikipedia.png',\n",
              "  'keywords': '',\n",
              "  'lang': 'en'},\n",
              " 'movies': [],\n",
              " 'opengraph': {'title': 'Automatic summarization - Wikipedia',\n",
              "  'type': 'website'},\n",
              " 'publish_date': None,\n",
              " 'tags': [],\n",
              " 'title': 'Automatic summarization - Wikipedia',\n",
              " 'tweets': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Ls5slFpS3lCF",
        "outputId": "abba109a-bc46-484b-e224-af1b5fb2866d"
      },
      "source": [
        "article.title"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Automatic summarization - Wikipedia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "KIMcqB4m3rCq",
        "outputId": "4d49fb5e-90af-4164-ef86-b943c2a3129c"
      },
      "source": [
        "article.cleaned_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\\n\\nIn addition to text, images and videos can also be summarized. Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.[2]\\n\\nThere are two general approaches to automatic summarization: extraction and abstraction.\\n\\nHere, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[4]\\n\\nThis has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.\\n\\nApproaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\\n\\nThere are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\n\\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\\n\\nImage collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\\n\\nAt a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\\n\\nThe task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.[6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below. Consider the example text from a news article:\\n\\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system. Keyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\\n\\nDepending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.\\n\\nBeginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem. Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall. Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\\n\\nDesigning a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\\n\\nWe also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.\\n\\nIn the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\\n\\nOnce examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney\\'s GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\\n\\nAnother keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney\\'s results demonstrate. Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text\\'s intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\\n\\nTextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\\n\\nThe vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\\n\\nEdges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\\n\\nSince this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\\n\\nIt is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\\n\\nIn short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\\n\\nLike keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.\\n\\nBefore getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated. The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.\\n\\nIf there are multiple references, the ROUGE-1 scores are averaged. Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner. High-order n-gram ROUGE measures try to judge fluency to some degree. Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.\\n\\nA promising line in document summarization is adaptive document/text summarization.[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre. First summarizes that perform adaptive summarization have been created.[10]\\n\\nSupervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.\\n\\nDuring the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\\n\\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\\n\\nA more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\\n\\nIn both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\\n\\nThe edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences\\' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.\\n\\nIn both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\\n\\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\\n\\nAnother important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization. The task remains the same in both cases—only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary. The method used is called Cross-Sentence Information Subsumption (CSIS).\\n\\nThese methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. The importance of this sentence also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\\n\\nMulti-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.[12] [4]\\n\\nMulti-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.[dubious – discuss]\\n\\nMulti-document extractive summarization faces a problem of potential redundancy. Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results. There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks. (An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.) The algorithm is called GRASSHOPPER.[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\\n\\nThe state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[16]\\n\\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed. This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).\\n\\nThe idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\\n\\nWhile submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee.[17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\\n\\nSubmodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.[citation needed]\\n\\nSubmodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.[22]\\n• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts. It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.[24] The name is reference to TL;DR − Internet slang for \"too long; didn\\'t read\".[25][26]\\n\\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\\n\\nEvaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual.[28]\\n\\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\\n\\nIntra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.\\n\\nHuman judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.\\n\\nOne of the metrics used in NIST\\'s annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]). It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary\\'s coherence. Anaphor resolution remains another problem yet to be fully solved. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.[29]\\n\\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments. Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text. For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.[30]\\n\\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries. Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\\n\\nThe first publication in the area dates back to 1958 (Lun), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization.[31]\\n• None Roxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization.\\n• None Elena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation.\\n• None Alrehamy, Hassan (2017). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". Automatic Keyphrases Extraction. Advances in Intelligent Systems and Computing. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN .\\n• None Marcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. ISBN .\\n• , Published in Proceeding RIAO\\'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\\n• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks . , The GRASSHOPPER algorithm\\n• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). \"Summarizing Conceptual Graphs for Automatic Summarization Task\". Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN . , Conceptual Structures for STEM Research and Education.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvkOagC43y6P",
        "outputId": "564d6fff-190e-4337-d00f-433c0b435c69"
      },
      "source": [
        "len(article.cleaned_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "K7wPXfhQ33IF",
        "outputId": "26823032-4e31-47ca-937f-c7ec5f736084"
      },
      "source": [
        "formatted_article = preprocess(article.cleaned_text)\n",
        "formatted_article"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"automatic summarization process shortening set data computationally create subset summary represents important relevant information within original content addition text images videos also summarized text summarization finds informative sentences document 1 image summarization finds representative images within image collection citation needed video summarization extracts important frames video content 2 two general approaches automatic summarization extraction abstraction content extracted original data extracted content modified way examples extracted content include key-phrases used `` tag '' index text document key sentences including headings collectively comprise abstract representative images video segments stated text extraction analogous process skimming summary available headings subheadings figures first last paragraphs section optionally first last sentences paragraph read one chooses read entire document detail 3 examples extraction include key sequences text terms clinical relevance including patient/problem intervention outcome 4 applied mainly text abstractive methods build internal semantic representation original content use representation create summary closer human might express abstraction may transform extracted content paraphrasing sections source document condense text strongly extraction transformation however computationally much challenging extraction involving natural language processing often deep understanding domain original text cases original document relates special field knowledge `` paraphrasing '' even difficult apply image video summarization systems extractive approaches aimed higher summarization quality rely combined software human effort machine aided human summarization extractive techniques highlight candidate passages inclusion human adds removes text human aided machine summarization human post-processes software output way one edits output automatic translation google translate broadly two types extractive summarization tasks depending summarization program focuses first generic summarization focuses obtaining generic summary abstract collection whether documents sets images videos news stories etc. second query relevant summarization sometimes called query-based summarization summarizes objects specific query summarization systems able create query relevant text summaries generic machine-generated summaries depending user needs example summarization problem document summarization attempts automatically produce abstract given document sometimes one might interested generating summary single source document others use multiple source documents example cluster articles topic problem called multi-document summarization related application summarizing news articles imagine system automatically pulls together news articles given topic web concisely represents latest news summary image collection summarization another application example automatic summarization consists selecting representative set images larger set images 5 summary context useful show representative images results image collection exploration system video summarization related domain system automatically creates trailer long video also applications consumer personal videos one might want skip boring repetitive actions similarly surveillance videos one would want extract important suspicious activity ignoring boring redundant frames captured high level summarization algorithms try find subsets objects like set sentences set images cover information entire set also called core-set algorithms model notions like diversity coverage information representativeness summary query based summarization techniques additionally model relevance summary query techniques algorithms naturally model summarization problems textrank pagerank submodular set function determinantal point process maximal marginal relevance mmr etc task following given piece text journal article must produce list keywords key phrase capture primary topics discussed text 6 case research articles many authors provide manually assigned keywords text lacks pre-existing keyphrases example news articles rarely keyphrases attached would useful able automatically number applications discussed consider example text news article keyphrase extractor might select `` army corps engineers '' `` president bush '' `` new orleans '' `` defective flood-control pumps '' keyphrases pulled directly text contrast abstractive keyphrase system would somehow internalize content generate keyphrases appear text closely resemble human might produce `` political negligence '' `` inadequate protection floods '' abstraction requires deep understanding text makes difficult computer system keyphrases many applications enable document browsing providing short summary improve information retrieval documents keyphrases assigned user could search keyphrase produce reliable hits full-text search employed generating index entries large text corpus depending different literature definition key terms words phrases keyword extraction highly related theme beginning work turney 7 many researchers approached keyphrase extraction supervised machine learning problem given document construct example unigram bigram trigram found text though text units also possible discussed compute various features describing example e.g. phrase begin upper-case letter assume known keyphrases available set training documents using known keyphrases assign positive negative labels examples learn classifier discriminate positive negative examples function features classifiers make binary classification test example others assign probability keyphrase instance text might learn rule says phrases initial capital letters likely keyphrases training learner select keyphrases test documents following manner apply example-generation strategy test documents run example learner determine keyphrases looking binary classification decisions probabilities returned learned model probabilities given threshold used select keyphrases keyphrase extractors generally evaluated using precision recall precision measures many proposed keyphrases actually correct recall measures many true keyphrases system proposed two measures combined f-score harmonic mean two f 2pr/ p r matches proposed keyphrases known keyphrases checked stemming applying text normalization designing supervised keyphrase extraction system involves deciding several choices apply unsupervised first choice exactly generate examples turney others used possible unigrams bigrams trigrams without intervening punctuation removing stopwords hulth showed get improvement selecting examples sequences tokens match certain patterns part-of-speech tags ideally mechanism generating examples produces known labeled keyphrases candidates though often case example use unigrams bigrams trigrams never able extract known keyphrase containing four words thus recall may suffer however generating many examples also lead low precision also need create features describe examples informative enough allow learning algorithm discriminate keyphrases non- keyphrases typically features involve various term frequencies many times phrase appears current text larger corpus length example relative position first occurrence various boolean syntactic features e.g. contains caps etc turney paper used 12 features hulth uses reduced set features found successful kea keyphrase extraction algorithm work derived turney ’ seminal paper end system need return list keyphrases test document need way limit number ensemble methods i.e. using votes several classifiers used produce numeric scores thresholded provide user-provided number keyphrases technique used turney c4.5 decision trees hulth used single binary classifier learning algorithm implicitly determines appropriate number examples features created need way learn predict keyphrases virtually supervised learning algorithm could used decision trees naive bayes rule induction case turney 's genex algorithm genetic algorithm used learn parameters domain-specific keyphrase extraction algorithm extractor follows series heuristics identify keyphrases genetic algorithm optimizes parameters heuristics respect performance training documents known key phrases another keyphrase extraction algorithm textrank supervised methods nice properties like able produce interpretable rules features characterize keyphrase also require large amount training data many documents known keyphrases needed furthermore training specific domain tends customize extraction process domain resulting classifier necessarily portable turney 's results demonstrate unsupervised keyphrase extraction removes need training data approaches problem different angle instead trying learn explicit features characterize keyphrases textrank algorithm 8 exploits structure text determine keyphrases appear `` central '' text way pagerank selects important web pages recall based notion `` prestige '' `` recommendation '' social networks way textrank rely previous training data rather run arbitrary piece text produce output simply based text 's intrinsic properties thus algorithm easily portable new domains languages textrank general purpose graph-based ranking algorithm nlp essentially runs pagerank graph specially designed particular nlp task keyphrase extraction builds graph using set text units vertices edges based measure semantic lexical similarity text unit vertices unlike pagerank edges typically undirected weighted reflect degree similarity graph constructed used form stochastic matrix combined damping factor `` random surfer model '' ranking vertices obtained finding eigenvector corresponding eigenvalue 1 i.e. stationary distribution random walk graph vertices correspond want rank potentially could something similar supervised methods create vertex unigram bigram trigram etc however keep graph small authors decide rank individual unigrams first step include second step merges highly ranked adjacent unigrams form multi-word phrases nice side effect allowing us produce keyphrases arbitrary length example rank unigrams find `` advanced '' `` natural '' `` language '' `` processing '' get high ranks would look original text see words appear consecutively create final keyphrase using four together note unigrams placed graph filtered part speech authors found adjectives nouns best include thus linguistic knowledge comes play step edges created based word co-occurrence application textrank two vertices connected edge unigrams appear within window size n original text n typically around 2–10 thus `` natural '' `` language '' might linked text nlp `` natural '' `` processing '' would also linked would appear string n words edges build notion `` text cohesion '' idea words appear near likely related meaningful way `` recommend '' reader since method simply ranks individual vertices need way threshold produce limited number keyphrases technique chosen set count user-specified fraction total number vertices graph top vertices/unigrams selected based stationary probabilities post- processing step applied merge adjacent instances unigrams result potentially less final keyphrases produced number roughly proportional length original text initially clear applying pagerank co-occurrence graph would produce useful keyphrases one way think following word appears multiple times throughout text may many different co-occurring neighbors example text machine learning unigram `` learning '' might co-occur `` machine '' `` supervised '' `` un-supervised '' `` semi-supervised '' four different sentences thus `` learning '' vertex would central `` hub '' connects modifying words running pagerank/textrank graph likely rank `` learning '' highly similarly text contains phrase `` supervised classification '' would edge `` supervised '' `` classification '' `` classification '' appears several places thus many neighbors importance would contribute importance `` supervised '' ends high rank selected one top unigrams along `` learning '' probably `` classification '' final post-processing step would end keyphrases `` supervised learning '' `` supervised classification '' short co-occurrence graph contain densely connected regions terms appear often different contexts random walk graph stationary distribution assigns large probabilities terms centers clusters similar densely connected web pages getting ranked highly pagerank approach also used document summarization considered like keyphrase extraction document summarization aims identify essence text real difference dealing larger text units—whole sentences instead words phrases getting details summarization methods mention summarization systems typically evaluated common way using so-called rouge recall-oriented understudy gisting evaluation measure recall-based measure determines well system-generated summary covers content present one human-generated model summaries known references recall-based encourage systems include important topics text recall computed respect unigram bigram trigram 4-gram matching example rouge-1 computed division count unigrams reference appear system count unigrams reference summary multiple references rouge-1 scores averaged rouge based content overlap determine general concepts discussed automatic summary reference summary determine result coherent sentences flow together sensible manner high-order n-gram rouge measures try judge fluency degree note rouge similar bleu measure machine translation bleu precision- based translation systems favor accuracy promising line document summarization adaptive document/text summarization 9 idea adaptive summarization involves preliminary recognition document/text genre subsequent application summarization algorithms optimized genre first summarizes perform adaptive summarization created 10 supervised text summarization much like supervised keyphrase extraction basically collection documents human-generated summaries learn features sentences make good candidates inclusion summary features might include position document i.e. first sentences probably important number words sentence etc main difficulty supervised extractive summarization known summaries must manually created extracting sentences sentences original training document labeled `` summary '' `` summary '' typically people create summaries simply using journal abstracts existing summaries usually sufficient sentences summaries necessarily match sentences original text would difficult assign labels examples training note however natural summaries still used evaluation purposes since rouge-1 cares unigrams duc 2001 2002 evaluation workshops tno developed sentence extraction system multi-document summarization news domain system based hybrid system using naive bayes classifier statistical language models modeling salience although system exhibited good results researchers wanted explore effectiveness maximum entropy classifier meeting summarization task known robust feature dependencies maximum entropy also applied successfully summarization broadcast news domain unsupervised approach summarization also quite similar spirit unsupervised keyphrase extraction gets around issue costly training data unsupervised summarization approaches based finding `` centroid '' sentence mean word vector sentences document sentences ranked regard similarity centroid sentence principled way estimate sentence importance using random walks eigenvector centrality lexrank 11 algorithm essentially identical textrank use approach document summarization two methods developed different groups time lexrank simply focused summarization could easily used keyphrase extraction nlp ranking task lexrank textrank graph constructed creating vertex sentence document edges sentences based form semantic similarity content overlap lexrank uses cosine similarity tf-idf vectors textrank uses similar measure based number words two sentences common normalized sentences lengths lexrank paper explored using unweighted edges applying threshold cosine values also experimented using edges weights equal similarity score textrank uses continuous similarity scores weights algorithms sentences ranked applying pagerank resulting graph summary formed combining top ranking sentences using threshold length cutoff limit size summary worth noting textrank applied summarization exactly described lexrank used part larger summarization system mead combines lexrank score stationary probability features like sentence position length using linear combination either user-specified automatically tuned weights case training documents might needed though textrank results show additional features absolutely necessary another important distinction textrank used single document summarization lexrank applied multi-document summarization task remains cases—only number sentences choose grown however summarizing multiple documents greater risk selecting duplicate highly redundant sentences place summary imagine cluster news articles particular event want produce one summary article likely many similar sentences would want include distinct ideas summary address issue lexrank applies heuristic post-processing step builds summary adding sentences rank order discards sentences similar ones already placed summary method used called cross-sentence information subsumption csis methods work based idea sentences `` recommend '' similar sentences reader thus one sentence similar many others likely sentence great importance importance sentence also stems importance sentences `` recommending '' thus get ranked highly placed summary sentence must similar many sentences turn also similar many sentences makes intuitive sense allows algorithms applied arbitrary new text methods domain-independent easily portable one could imagine features indicating important sentences news domain might vary considerably biomedical domain however unsupervised `` recommendation '' -based approach applies domain multi-document summarization automatic procedure aimed extraction information multiple texts written topic resulting summary report allows individual users professional information consumers quickly familiarize information contained large cluster documents way multi-document summarization systems complementing news aggregators performing next step road coping information overload multi-document summarization may also done response question 12 4 multi-document summarization creates information reports concise comprehensive different opinions put together outlined every topic described multiple perspectives within single document goal brief summary simplify information search cut time pointing relevant source documents comprehensive multi-document summary contain required information hence limiting need accessing original files cases refinement required automatic summaries present information extracted multiple sources algorithmically without editorial touch subjective human intervention thus making completely unbiased dubious – discuss multi-document extractive summarization faces problem potential redundancy ideally would like extract sentences `` central '' i.e. contain main ideas `` diverse '' i.e. differ one another lexrank deals diversity heuristic final stage using csis systems used similar methods maximal marginal relevance mmr 13 trying eliminate redundancy information retrieval results general purpose graph-based ranking algorithm like page/lex/textrank handles `` centrality '' `` diversity '' unified mathematical framework based absorbing markov chain random walks absorbing random walk like standard random walk except states absorbing states act `` black holes '' cause walk end abruptly state algorithm called grasshopper 14 addition explicitly promoting diversity ranking process grasshopper incorporates prior ranking based sentence position case summarization state art results multi-document summarization however obtained using mixtures submodular functions methods achieved state art results document summarization corpora duc 04 07 15 similar results also achieved use determinantal point processes special case submodular functions duc-04 16 new method multi-lingual multi-document summarization avoids redundancy works simplifying generating ideograms represent meaning sentence document evaluates similarity `` qualitatively '' comparing shape position said ideograms recently developed tool use word frequency need training preprocessing kind works generating ideograms represent meaning sentence summarizes using two user-supplied parameters equivalence two sentences considered equivalent relevance long desired summary idea submodular set function recently emerged powerful modeling tool various summarization problems submodular functions naturally model notions coverage information representation diversity moreover several important combinatorial optimization problems occur special instances submodular optimization example set cover problem special case submodular optimization since set cover function submodular set cover function attempts find subset objects cover given set concepts example document summarization one would like summary cover important relevant concepts document instance set cover similarly facility location problem special case submodular functions facility location function also naturally models coverage diversity another example submodular optimization problem using determinantal point process model diversity similarly maximum-marginal-relevance procedure also seen instance submodular optimization important models encouraging coverage diversity information submodular moreover submodular functions efficiently combined together resulting function still submodular hence one could combine one submodular function models diversity another one models coverage use human supervision learn right model submodular function problem submodular functions fitting problems summarization also admit efficient algorithms optimization example simple greedy algorithm admits constant factor guarantee 17 moreover greedy algorithm extremely simple implement scale large datasets important summarization problems submodular functions achieved state-of-the-art almost summarization problems example work lin bilmes 2012 18 shows submodular functions achieve best results date duc-04 duc-05 duc-06 duc-07 systems document summarization similarly work lin bilmes 2011 19 shows many existing systems automatic summarization instances submodular functions breakthrough result establishing submodular functions right models summarization problems citation needed submodular functions also used summarization tasks tschiatschek et al. 2014 show 20 mixtures submodular functions achieve state-of-the-art results image collection summarization similarly bairi et al. 2015 21 show utility submodular functions summarizing multi-document topic hierarchies submodular functions also successfully used summarizing machine learning datasets 22 • reddit bot `` autotldr '' 23 created 2011 summarizes news articles comment-section reddit posts found useful reddit community upvoted summaries hundreds thousands times 24 name reference tl dr − internet slang `` long n't read '' 25 26 common way evaluate informativeness automatic summaries compare human-made model summaries evaluation techniques fall intrinsic extrinsic 27 inter-textual intra-textual 28 intrinsic evaluation tests summarization system extrinsic evaluation tests summarization based affects completion task intrinsic evaluations assessed mainly coherence informativeness summaries extrinsic evaluations hand tested impact summarization tasks like relevance assessment reading comprehension etc intra-textual methods assess output specific summarization system inter-textual ones focus contrastive analysis outputs several summarization systems human judgement often wide variance considered `` good '' summary means making evaluation process automatic particularly difficult manual evaluation used time labor-intensive requires humans read summaries also source documents issues concerning coherence coverage one metrics used nist 's annual document understanding conferences research groups submit systems summarization translation tasks rouge metric recall-oriented understudy gisting evaluation 2 essentially calculates n-gram overlaps automatically generated summaries previously-written human summaries high level overlap indicate high level shared concepts two summaries note overlap metrics like unable provide feedback summary 's coherence anaphor resolution remains another problem yet fully solved similarly image summarization tschiatschek et al. developed visual-rouge score judges performance algorithms image summarization 29 domain independent summarization techniques generally apply sets general features used identify information-rich text segments recent research focus drifted domain-specific summarization techniques utilize available knowledge specific domain text example automatic summarization research medical text generally attempts utilize various sources codified medical knowledge ontologies 30 main drawback evaluation systems existing far need least one reference summary methods one able compare automatic summaries models hard expensive task much effort done order corpus texts corresponding summaries furthermore methods need human-made summaries available comparison also manual annotation performed e.g scu pyramid method case evaluation methods need input set summaries serve gold standards set automatic summaries moreover perform quantitative evaluation regard different similarity metrics first publication area dates back 1958 lun starting statistical technique research increased significantly 2015. term frequency–inverse document frequency used 2016. pattern-based summarization powerful option multi-document summarization found 2016. following year surpassed latent semantic analysis lsa combined non-negative matrix factorization nmf although replace approaches often combined 2019 machine learning methods dominated extractive summarization single documents considered nearing maturity 2020 field still active research shifting towards abstractive summation real-time summarization 31 • none roxana angheluta 2002 use topic segmentation automatic summarization • none elena lloret manuel palomar 2009 challenging issues automatic summarization relevance detection quality-based evaluation • none alrehamy hassan 2017 `` semcluster unsupervised automatic keyphrase extraction using affinity propagation '' automatic keyphrases extraction advances intelligent systems computing 650. pp 222–235 doi:10.1007/978-3-319-66939-7_19 isbn • none marcu daniel 2000 theory practice discourse parsing summarization isbn • published proceeding riao'10 adaptivity personalization fusion heterogeneous information cid paris france • none xiaojin zhu andrew goldberg jurgen van gael david andrzejewski 2007 improving diversity ranking using absorbing random walks grasshopper algorithm • none miranda-jiménez sabino gelbukh alexander sidorov grigori 2013 `` summarizing conceptual graphs automatic summarization task '' conceptual structures stem research education lecture notes computer science 7735. pp 245–253 doi:10.1007/978-3-642-35786-2_18 isbn conceptual structures stem research education\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtTiTJD34_06",
        "outputId": "1fe4dd3a-35da-48a9-f721-afdcf5ba0a42"
      },
      "source": [
        "len(formatted_article)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eUAZ8h36WQE"
      },
      "source": [
        "def summarize(text, number_of_sentences, percentage = 0):\n",
        "  original_text = text\n",
        "  formatted_text = preprocess(original_text)\n",
        "\n",
        "  word_frequency = nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
        "  highest_frequency = max(word_frequency.values())\n",
        "  for word in word_frequency.keys():\n",
        "    word_frequency[word] = (word_frequency[word] / highest_frequency)\n",
        "  sentence_list = nltk.sent_tokenize(original_text)\n",
        "  \n",
        "  score_sentences = {}\n",
        "  for sentence in sentence_list:\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "      if word in word_frequency.keys():\n",
        "        if sentence not in score_sentences.keys():\n",
        "          score_sentences[sentence] = word_frequency[word]\n",
        "        else:\n",
        "          score_sentences[sentence] += word_frequency[word]\n",
        "\n",
        "  import heapq\n",
        "  if percentage > 0:\n",
        "    best_sentences = heapq.nlargest(int(len(sentence_list) * percentage), score_sentences, key=score_sentences.get)\n",
        "  else:\n",
        "    best_sentences = heapq.nlargest(number_of_sentences, score_sentences, key=score_sentences.get)\n",
        "\n",
        "  return sentence_list, best_sentences, word_frequency, score_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-Jot7Uy9I54",
        "outputId": "812966eb-9fe9-4605-ebef-08b10832c95a"
      },
      "source": [
        "len(sentence_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNqyufDp9XvL",
        "outputId": "4ffc7186-dccf-4ed1-e031-cc9e989bc286"
      },
      "source": [
        "(120 / len(sentence_list)) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.54054054054054"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCvPd38e7RDD"
      },
      "source": [
        "sentence_list, best_sentences, word_frequency, score_sentences = summarize(article.cleaned_text, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS9xOXwg7nAv",
        "outputId": "99d674eb-505a-4afe-ac16-6f05210fb37e"
      },
      "source": [
        "sentence_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.',\n",
              " 'In addition to text, images and videos can also be summarized.',\n",
              " 'Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.',\n",
              " '[2]\\n\\nThere are two general approaches to automatic summarization: extraction and abstraction.',\n",
              " 'Here, content is extracted from the original data, but the extracted content is not modified in any way.',\n",
              " 'Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.',\n",
              " 'For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.',\n",
              " '[3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).',\n",
              " '[4]\\n\\nThis has been applied mainly for text.',\n",
              " 'Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express.',\n",
              " 'Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction.',\n",
              " 'Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.',\n",
              " '\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.',\n",
              " 'Approaches aimed at higher summarization quality rely on combined software and human effort.',\n",
              " 'In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text).',\n",
              " 'In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.',\n",
              " 'There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.',\n",
              " 'The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).',\n",
              " 'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.',\n",
              " 'Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.',\n",
              " 'An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.',\n",
              " 'Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).',\n",
              " 'This problem is called multi-document summarization.',\n",
              " 'A related application is summarizing news articles.',\n",
              " 'Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.',\n",
              " 'Image collection summarization is another application example of automatic summarization.',\n",
              " 'It consists in selecting a representative set of images from a larger set of images.',\n",
              " '[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system.',\n",
              " 'Video summarization is a related domain, where the system automatically creates a trailer of a long video.',\n",
              " 'This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions.',\n",
              " 'Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.',\n",
              " 'At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.',\n",
              " 'This is also called the core-set.',\n",
              " 'These algorithms model notions like diversity, coverage, information and representativeness of the summary.',\n",
              " 'Query based summarization techniques, additionally model for relevance of the summary with the query.',\n",
              " 'Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.',\n",
              " 'The task is the following.',\n",
              " 'You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.',\n",
              " '[6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases.',\n",
              " 'For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.',\n",
              " 'Consider the example text from a news article:\\n\\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases.',\n",
              " 'These are pulled directly from the text.',\n",
              " 'In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".',\n",
              " 'Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.',\n",
              " 'Keyphrases have many applications.',\n",
              " 'They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.',\n",
              " 'Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.',\n",
              " 'Beginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem.',\n",
              " 'Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below).',\n",
              " 'We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?).',\n",
              " 'We assume there are known keyphrases available for a set of training documents.',\n",
              " 'Using the known keyphrases, we can assign positive or negative labels to the examples.',\n",
              " 'Then we learn a classifier that can discriminate between positive and negative examples as a function of the features.',\n",
              " 'Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase.',\n",
              " 'For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.',\n",
              " 'After training a learner, we can select keyphrases for test documents in the following manner.',\n",
              " 'We apply the same example-generation strategy to the test documents, then run each example through the learner.',\n",
              " 'We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model.',\n",
              " 'If probabilities are given, a threshold is used to select the keyphrases.',\n",
              " 'Keyphrase extractors are generally evaluated using precision and recall.',\n",
              " 'Precision measures how many of the proposed keyphrases are actually correct.',\n",
              " 'Recall measures how many of the true keyphrases your system proposed.',\n",
              " 'The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ).',\n",
              " 'Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.',\n",
              " 'Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too).',\n",
              " 'The first choice is exactly how to generate examples.',\n",
              " 'Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords.',\n",
              " 'Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags.',\n",
              " 'Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case.',\n",
              " 'For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.',\n",
              " 'Thus, recall may suffer.',\n",
              " 'However, generating too many examples can also lead to low precision.',\n",
              " 'We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.',\n",
              " 'Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.',\n",
              " 'The Turney paper used about 12 such features.',\n",
              " 'Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.',\n",
              " 'In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number.',\n",
              " 'Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases.',\n",
              " 'This is the technique used by Turney with C4.5 decision trees.',\n",
              " 'Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.',\n",
              " 'Once examples and features are created, we need a way to learn to predict keyphrases.',\n",
              " 'Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction.',\n",
              " \"In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.\",\n",
              " 'The extractor follows a series of heuristics to identify keyphrases.',\n",
              " 'The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.',\n",
              " 'Another keyphrase extraction algorithm is TextRank.',\n",
              " 'While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.',\n",
              " 'Many documents with known keyphrases are needed.',\n",
              " \"Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\",\n",
              " 'Unsupervised keyphrase extraction removes the need for training data.',\n",
              " 'It approaches the problem from a different angle.',\n",
              " 'Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.',\n",
              " 'Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks.',\n",
              " \"In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.\",\n",
              " 'Thus the algorithm is easily portable to new domains and languages.',\n",
              " 'TextRank is a general purpose graph-based ranking algorithm for NLP.',\n",
              " 'Essentially, it runs PageRank on a graph specially designed for a particular NLP task.',\n",
              " 'For keyphrase extraction, it builds a graph using some set of text units as vertices.',\n",
              " 'Edges are based on some measure of semantic or lexical similarity between the text unit vertices.',\n",
              " 'Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity.',\n",
              " 'Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).',\n",
              " 'The vertices should correspond to what we want to rank.',\n",
              " 'Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc.',\n",
              " 'However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases.',\n",
              " 'This has a nice side effect of allowing us to produce keyphrases of arbitrary length.',\n",
              " 'For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.',\n",
              " 'Note that the unigrams placed in the graph can be filtered by part of speech.',\n",
              " 'The authors found that adjectives and nouns were the best to include.',\n",
              " 'Thus, some linguistic knowledge comes into play in this step.',\n",
              " 'Edges are created based on word co-occurrence in this application of TextRank.',\n",
              " 'Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text.',\n",
              " 'N is typically around 2–10.',\n",
              " 'Thus, \"natural\" and \"language\" might be linked in a text about NLP.',\n",
              " '\"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words.',\n",
              " 'These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.',\n",
              " 'Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases.',\n",
              " 'The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph.',\n",
              " 'Then the top T vertices/unigrams are selected based on their stationary probabilities.',\n",
              " 'A post- processing step is then applied to merge adjacent instances of these T unigrams.',\n",
              " 'As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.',\n",
              " 'It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases.',\n",
              " 'One way to think about it is the following.',\n",
              " 'A word that appears multiple times throughout a text may have many different co-occurring neighbors.',\n",
              " 'For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences.',\n",
              " 'Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words.',\n",
              " 'Running PageRank/TextRank on the graph is likely to rank \"learning\" highly.',\n",
              " 'Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\".',\n",
              " 'If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\".',\n",
              " 'If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\".',\n",
              " 'In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".',\n",
              " 'In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts.',\n",
              " 'A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters.',\n",
              " 'This is similar to densely connected Web pages getting ranked highly by PageRank.',\n",
              " 'This approach has also been used in document summarization, considered below.',\n",
              " 'Like keyphrase extraction, document summarization aims to identify the essence of a text.',\n",
              " 'The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.',\n",
              " 'Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.',\n",
              " 'The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.',\n",
              " 'This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references.',\n",
              " 'It is recall-based to encourage systems to include all the important topics in the text.',\n",
              " 'Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching.',\n",
              " 'For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.',\n",
              " 'If there are multiple references, the ROUGE-1 scores are averaged.',\n",
              " 'Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.',\n",
              " 'High-order n-gram ROUGE measures try to judge fluency to some degree.',\n",
              " 'Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.',\n",
              " 'A promising line in document summarization is adaptive document/text summarization.',\n",
              " '[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.',\n",
              " 'First summarizes that perform adaptive summarization have been created.',\n",
              " '[10]\\n\\nSupervised text summarization is very much like supervised keyphrase extraction.',\n",
              " 'Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.',\n",
              " 'Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.',\n",
              " 'The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".',\n",
              " 'This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient.',\n",
              " 'The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.',\n",
              " 'Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.',\n",
              " 'During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.',\n",
              " 'The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience.',\n",
              " 'Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies.',\n",
              " 'Maximum entropy has also been applied successfully for summarization in the broadcast news domain.',\n",
              " 'The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.',\n",
              " 'Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.',\n",
              " 'Then the sentences can be ranked with regard to their similarity to this centroid sentence.',\n",
              " 'A more principled way to estimate sentence importance is using random walks and eigenvector centrality.',\n",
              " 'LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.',\n",
              " 'The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.',\n",
              " 'In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.',\n",
              " 'The edges between sentences are based on some form of semantic similarity or content overlap.',\n",
              " \"While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).\",\n",
              " 'The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score.',\n",
              " 'TextRank uses continuous similarity scores as weights.',\n",
              " 'In both algorithms, the sentences are ranked by applying PageRank to the resulting graph.',\n",
              " 'A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.',\n",
              " 'It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.',\n",
              " 'In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.',\n",
              " 'Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization.',\n",
              " 'The task remains the same in both cases—only the number of sentences to choose from has grown.',\n",
              " 'However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary.',\n",
              " 'Imagine you have a cluster of news articles on a particular event, and you want to produce one summary.',\n",
              " 'Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary.',\n",
              " 'To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.',\n",
              " 'The method used is called Cross-Sentence Information Subsumption (CSIS).',\n",
              " 'These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader.',\n",
              " 'Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance.',\n",
              " 'The importance of this sentence also stems from the importance of the sentences \"recommending\" it.',\n",
              " 'Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.',\n",
              " 'This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text.',\n",
              " 'The methods are domain-independent and easily portable.',\n",
              " 'One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain.',\n",
              " 'However, the unsupervised \"recommendation\"-based approach applies to any domain.',\n",
              " 'Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.',\n",
              " 'Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents.',\n",
              " 'In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.',\n",
              " 'Multi-document summarization may also be done in response to a question.',\n",
              " '[12] [4]\\n\\nMulti-document summarization creates information reports that are both concise and comprehensive.',\n",
              " 'With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document.',\n",
              " 'While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.',\n",
              " 'Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.',\n",
              " '[dubious – discuss]\\n\\nMulti-document extractive summarization faces a problem of potential redundancy.',\n",
              " 'Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another).',\n",
              " 'LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results.',\n",
              " 'There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.',\n",
              " '(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.)',\n",
              " 'The algorithm is called GRASSHOPPER.',\n",
              " '[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).',\n",
              " 'The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions.',\n",
              " 'These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.',\n",
              " '[15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.',\n",
              " '[16]\\n\\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.',\n",
              " 'This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).',\n",
              " 'The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.',\n",
              " 'Submodular functions naturally model notions of coverage, information, representation and diversity.',\n",
              " 'Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization.',\n",
              " 'For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.',\n",
              " 'The set cover function attempts to find a subset of objects which cover a given set of concepts.',\n",
              " 'For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.',\n",
              " 'This is an instance of set cover.',\n",
              " 'Similarly, the facility location problem is a special case of submodular functions.',\n",
              " 'The Facility Location function also naturally models coverage and diversity.',\n",
              " 'Another example of a submodular optimization problem is using a determinantal point process to model diversity.',\n",
              " 'Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization.',\n",
              " 'All these important models encouraging coverage, diversity and information are all submodular.',\n",
              " 'Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular.',\n",
              " 'Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.',\n",
              " 'While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization.',\n",
              " 'For example, a simple greedy algorithm admits a constant factor guarantee.',\n",
              " '[17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.',\n",
              " 'Submodular functions have achieved state-of-the-art for almost all summarization problems.',\n",
              " 'For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.',\n",
              " 'Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions.',\n",
              " 'This was a breakthrough result establishing submodular functions as the right models for summarization problems.',\n",
              " '[citation needed]\\n\\nSubmodular Functions have also been used for other summarization tasks.',\n",
              " 'Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization.',\n",
              " 'Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies.',\n",
              " 'Submodular Functions have also successfully been used for summarizing machine learning datasets.',\n",
              " '[22]\\n• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts.',\n",
              " 'It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.',\n",
              " '[24] The name is reference to TL;DR − Internet slang for \"too long; didn\\'t read\".',\n",
              " '[25][26]\\n\\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.',\n",
              " 'Evaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual.',\n",
              " '[28]\\n\\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.',\n",
              " 'Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries.',\n",
              " 'Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.',\n",
              " 'Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.',\n",
              " 'Human judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult.',\n",
              " 'Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents.',\n",
              " 'Other issues are those concerning coherence and coverage.',\n",
              " \"One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]).\",\n",
              " 'It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries.',\n",
              " 'A high level of overlap should indicate a high level of shared concepts between the two summaries.',\n",
              " \"Note that overlap metrics like this are unable to provide any feedback on a summary's coherence.\",\n",
              " 'Anaphor resolution remains another problem yet to be fully solved.',\n",
              " 'Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.',\n",
              " '[29]\\n\\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments.',\n",
              " 'Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.',\n",
              " 'For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.',\n",
              " '[30]\\n\\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.',\n",
              " 'This is a hard and expensive task.',\n",
              " 'Much effort has to be done in order to have corpus of texts and their corresponding summaries.',\n",
              " 'Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g.',\n",
              " 'SCU in the Pyramid Method).',\n",
              " 'In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries.',\n",
              " 'Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.',\n",
              " 'The first publication in the area dates back to 1958 (Lun), starting with a statistical technique.',\n",
              " 'Research increased significantly in 2015.',\n",
              " 'Term frequency–inverse document frequency had been used by 2016.',\n",
              " 'Pattern-based summarization was the most powerful option for multi-document summarization found by 2016.',\n",
              " 'In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF).',\n",
              " 'Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.',\n",
              " 'By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization.',\n",
              " '[31]\\n• None Roxana, Angheluta (2002).',\n",
              " 'The Use of Topic Segmentation for Automatic Summarization.',\n",
              " '• None Elena, Lloret and Manuel, Palomar (2009).',\n",
              " 'Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation.',\n",
              " '• None Alrehamy, Hassan (2017).',\n",
              " '\"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\".',\n",
              " 'Automatic Keyphrases Extraction.',\n",
              " 'Advances in Intelligent Systems and Computing.',\n",
              " '650. pp.',\n",
              " '222–235.',\n",
              " 'doi:10.1007/978-3-319-66939-7_19.',\n",
              " 'ISBN .',\n",
              " '• None Marcu, Daniel (2000).',\n",
              " 'The Theory and Practice of Discourse Parsing and Summarization.',\n",
              " 'ISBN .',\n",
              " \"• , Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\\n• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007).\",\n",
              " 'Improving diversity in ranking using absorbing random walks .',\n",
              " ', The GRASSHOPPER algorithm\\n• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013).',\n",
              " '\"Summarizing Conceptual Graphs for Automatic Summarization Task\".',\n",
              " 'Conceptual Structures for STEM Research and Education.',\n",
              " 'Lecture Notes in Computer Science.',\n",
              " '7735. pp.',\n",
              " '245–253.',\n",
              " 'doi:10.1007/978-3-642-35786-2_18.',\n",
              " 'ISBN .',\n",
              " ', Conceptual Structures for STEM Research and Education.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRp9qC3f8BEp",
        "outputId": "cb1153cf-14af-4980-a519-b81d28c89cd2"
      },
      "source": [
        "best_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences.',\n",
              " 'For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.',\n",
              " 'Consider the example text from a news article:\\n\\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases.',\n",
              " 'The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".',\n",
              " 'Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\".',\n",
              " 'In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".',\n",
              " 'Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.',\n",
              " 'Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.',\n",
              " 'In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".',\n",
              " 'These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.',\n",
              " 'Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another).',\n",
              " 'It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.',\n",
              " 'There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.',\n",
              " 'Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.',\n",
              " '[16]\\n\\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.',\n",
              " 'If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\".',\n",
              " 'Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.',\n",
              " '\"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words.',\n",
              " 'If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\".',\n",
              " 'Thus, \"natural\" and \"language\" might be linked in a text about NLP.',\n",
              " 'An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.',\n",
              " 'Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words.',\n",
              " 'For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.',\n",
              " 'Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization.',\n",
              " '[28]\\n\\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.',\n",
              " 'They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.',\n",
              " 'Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks.',\n",
              " 'At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.',\n",
              " 'Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).',\n",
              " '\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.',\n",
              " 'For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.',\n",
              " 'Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.',\n",
              " 'These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader.',\n",
              " 'Image collection summarization is another application example of automatic summarization.',\n",
              " 'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.',\n",
              " 'The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.',\n",
              " 'Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.',\n",
              " 'Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.',\n",
              " 'For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.',\n",
              " 'Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.',\n",
              " 'The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.',\n",
              " 'Human judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult.',\n",
              " 'Like keyphrase extraction, document summarization aims to identify the essence of a text.',\n",
              " 'Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.',\n",
              " 'The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).',\n",
              " 'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.',\n",
              " 'A promising line in document summarization is adaptive document/text summarization.',\n",
              " '[10]\\n\\nSupervised text summarization is very much like supervised keyphrase extraction.',\n",
              " 'For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.',\n",
              " '[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.',\n",
              " 'This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).',\n",
              " '[29]\\n\\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments.',\n",
              " 'The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.',\n",
              " 'There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.',\n",
              " 'The importance of this sentence also stems from the importance of the sentences \"recommending\" it.',\n",
              " '[30]\\n\\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.',\n",
              " 'During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.',\n",
              " 'Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below).',\n",
              " 'Pattern-based summarization was the most powerful option for multi-document summarization found by 2016.',\n",
              " 'While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.',\n",
              " 'Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.',\n",
              " 'Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions.',\n",
              " 'Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).',\n",
              " 'Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.',\n",
              " 'The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions.',\n",
              " \"In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.\",\n",
              " '(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.)',\n",
              " 'To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.',\n",
              " 'Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.',\n",
              " 'Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.',\n",
              " 'Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.',\n",
              " 'While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization.',\n",
              " 'We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.',\n",
              " 'Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.',\n",
              " '[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).',\n",
              " 'This approach has also been used in document summarization, considered below.',\n",
              " 'In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.',\n",
              " 'For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.',\n",
              " 'The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.',\n",
              " 'Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization.',\n",
              " 'Query based summarization techniques, additionally model for relevance of the summary with the query.',\n",
              " 'Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.',\n",
              " '[22]\\n• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts.',\n",
              " 'Running PageRank/TextRank on the graph is likely to rank \"learning\" highly.',\n",
              " \"While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).\",\n",
              " 'In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries.',\n",
              " \"One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]).\",\n",
              " 'For keyphrase extraction, it builds a graph using some set of text units as vertices.',\n",
              " 'Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies.',\n",
              " '[2]\\n\\nThere are two general approaches to automatic summarization: extraction and abstraction.',\n",
              " 'While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.',\n",
              " \"In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.\",\n",
              " 'LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.',\n",
              " 'Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.',\n",
              " 'A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.',\n",
              " 'Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.',\n",
              " 'This was a breakthrough result establishing submodular functions as the right models for summarization problems.',\n",
              " 'Maximum entropy has also been applied successfully for summarization in the broadcast news domain.',\n",
              " 'You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.',\n",
              " '[citation needed]\\n\\nSubmodular Functions have also been used for other summarization tasks.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0udUNwhP8GV5",
        "outputId": "ec4e006c-7668-42f2-e73b-71f0456b1c36"
      },
      "source": [
        "word_frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'automatic': 0.21428571428571427,\n",
              "          'summarization': 1.0,\n",
              "          'process': 0.08333333333333333,\n",
              "          'shortening': 0.011904761904761904,\n",
              "          'set': 0.2261904761904762,\n",
              "          'data': 0.07142857142857142,\n",
              "          'computationally': 0.023809523809523808,\n",
              "          'create': 0.08333333333333333,\n",
              "          'subset': 0.023809523809523808,\n",
              "          'summary': 0.39285714285714285,\n",
              "          'represents': 0.023809523809523808,\n",
              "          'important': 0.14285714285714285,\n",
              "          'relevant': 0.05952380952380952,\n",
              "          'information': 0.20238095238095238,\n",
              "          'within': 0.047619047619047616,\n",
              "          'original': 0.13095238095238096,\n",
              "          'content': 0.13095238095238096,\n",
              "          'addition': 0.023809523809523808,\n",
              "          'text': 0.5476190476190477,\n",
              "          'images': 0.09523809523809523,\n",
              "          'videos': 0.047619047619047616,\n",
              "          'also': 0.27380952380952384,\n",
              "          'summarized': 0.011904761904761904,\n",
              "          'finds': 0.023809523809523808,\n",
              "          'informative': 0.023809523809523808,\n",
              "          'sentences': 0.39285714285714285,\n",
              "          'document': 0.3333333333333333,\n",
              "          '1': 0.023809523809523808,\n",
              "          'image': 0.09523809523809523,\n",
              "          'representative': 0.047619047619047616,\n",
              "          'collection': 0.07142857142857142,\n",
              "          'citation': 0.023809523809523808,\n",
              "          'needed': 0.047619047619047616,\n",
              "          'video': 0.07142857142857142,\n",
              "          'extracts': 0.011904761904761904,\n",
              "          'frames': 0.023809523809523808,\n",
              "          '2': 0.023809523809523808,\n",
              "          'two': 0.11904761904761904,\n",
              "          'general': 0.05952380952380952,\n",
              "          'approaches': 0.05952380952380952,\n",
              "          'extraction': 0.2619047619047619,\n",
              "          'abstraction': 0.03571428571428571,\n",
              "          'extracted': 0.05952380952380952,\n",
              "          'modified': 0.011904761904761904,\n",
              "          'way': 0.15476190476190477,\n",
              "          'examples': 0.13095238095238096,\n",
              "          'include': 0.08333333333333333,\n",
              "          'key-phrases': 0.011904761904761904,\n",
              "          'used': 0.27380952380952384,\n",
              "          '``': 0.6666666666666666,\n",
              "          'tag': 0.011904761904761904,\n",
              "          \"''\": 0.6666666666666666,\n",
              "          'index': 0.023809523809523808,\n",
              "          'key': 0.05952380952380952,\n",
              "          'including': 0.023809523809523808,\n",
              "          'headings': 0.023809523809523808,\n",
              "          'collectively': 0.011904761904761904,\n",
              "          'comprise': 0.011904761904761904,\n",
              "          'abstract': 0.03571428571428571,\n",
              "          'segments': 0.023809523809523808,\n",
              "          'stated': 0.011904761904761904,\n",
              "          'analogous': 0.011904761904761904,\n",
              "          'skimming': 0.011904761904761904,\n",
              "          'available': 0.047619047619047616,\n",
              "          'subheadings': 0.011904761904761904,\n",
              "          'figures': 0.011904761904761904,\n",
              "          'first': 0.10714285714285714,\n",
              "          'last': 0.023809523809523808,\n",
              "          'paragraphs': 0.011904761904761904,\n",
              "          'section': 0.011904761904761904,\n",
              "          'optionally': 0.011904761904761904,\n",
              "          'paragraph': 0.011904761904761904,\n",
              "          'read': 0.047619047619047616,\n",
              "          'one': 0.2261904761904762,\n",
              "          'chooses': 0.011904761904761904,\n",
              "          'entire': 0.023809523809523808,\n",
              "          'detail': 0.011904761904761904,\n",
              "          '3': 0.011904761904761904,\n",
              "          'sequences': 0.023809523809523808,\n",
              "          'terms': 0.047619047619047616,\n",
              "          'clinical': 0.011904761904761904,\n",
              "          'relevance': 0.08333333333333333,\n",
              "          'patient/problem': 0.011904761904761904,\n",
              "          'intervention': 0.023809523809523808,\n",
              "          'outcome': 0.011904761904761904,\n",
              "          '4': 0.023809523809523808,\n",
              "          'applied': 0.07142857142857142,\n",
              "          'mainly': 0.023809523809523808,\n",
              "          'abstractive': 0.03571428571428571,\n",
              "          'methods': 0.17857142857142858,\n",
              "          'build': 0.023809523809523808,\n",
              "          'internal': 0.011904761904761904,\n",
              "          'semantic': 0.047619047619047616,\n",
              "          'representation': 0.03571428571428571,\n",
              "          'use': 0.09523809523809523,\n",
              "          'closer': 0.011904761904761904,\n",
              "          'human': 0.13095238095238096,\n",
              "          'might': 0.13095238095238096,\n",
              "          'express': 0.011904761904761904,\n",
              "          'may': 0.047619047619047616,\n",
              "          'transform': 0.011904761904761904,\n",
              "          'paraphrasing': 0.023809523809523808,\n",
              "          'sections': 0.011904761904761904,\n",
              "          'source': 0.05952380952380952,\n",
              "          'condense': 0.011904761904761904,\n",
              "          'strongly': 0.011904761904761904,\n",
              "          'transformation': 0.011904761904761904,\n",
              "          'however': 0.08333333333333333,\n",
              "          'much': 0.03571428571428571,\n",
              "          'challenging': 0.023809523809523808,\n",
              "          'involving': 0.011904761904761904,\n",
              "          'natural': 0.05952380952380952,\n",
              "          'language': 0.047619047619047616,\n",
              "          'processing': 0.047619047619047616,\n",
              "          'often': 0.05952380952380952,\n",
              "          'deep': 0.023809523809523808,\n",
              "          'understanding': 0.03571428571428571,\n",
              "          'domain': 0.13095238095238096,\n",
              "          'cases': 0.023809523809523808,\n",
              "          'relates': 0.011904761904761904,\n",
              "          'special': 0.05952380952380952,\n",
              "          'field': 0.023809523809523808,\n",
              "          'knowledge': 0.047619047619047616,\n",
              "          'even': 0.011904761904761904,\n",
              "          'difficult': 0.047619047619047616,\n",
              "          'apply': 0.047619047619047616,\n",
              "          'systems': 0.15476190476190477,\n",
              "          'extractive': 0.07142857142857142,\n",
              "          'aimed': 0.023809523809523808,\n",
              "          'higher': 0.011904761904761904,\n",
              "          'quality': 0.011904761904761904,\n",
              "          'rely': 0.023809523809523808,\n",
              "          'combined': 0.07142857142857142,\n",
              "          'software': 0.023809523809523808,\n",
              "          'effort': 0.023809523809523808,\n",
              "          'machine': 0.09523809523809523,\n",
              "          'aided': 0.023809523809523808,\n",
              "          'techniques': 0.07142857142857142,\n",
              "          'highlight': 0.011904761904761904,\n",
              "          'candidate': 0.011904761904761904,\n",
              "          'passages': 0.011904761904761904,\n",
              "          'inclusion': 0.023809523809523808,\n",
              "          'adds': 0.011904761904761904,\n",
              "          'removes': 0.023809523809523808,\n",
              "          'post-processes': 0.011904761904761904,\n",
              "          'output': 0.047619047619047616,\n",
              "          'edits': 0.011904761904761904,\n",
              "          'translation': 0.047619047619047616,\n",
              "          'google': 0.011904761904761904,\n",
              "          'translate': 0.011904761904761904,\n",
              "          'broadly': 0.011904761904761904,\n",
              "          'types': 0.011904761904761904,\n",
              "          'tasks': 0.047619047619047616,\n",
              "          'depending': 0.03571428571428571,\n",
              "          'program': 0.011904761904761904,\n",
              "          'focuses': 0.023809523809523808,\n",
              "          'generic': 0.03571428571428571,\n",
              "          'obtaining': 0.011904761904761904,\n",
              "          'whether': 0.011904761904761904,\n",
              "          'documents': 0.17857142857142858,\n",
              "          'sets': 0.023809523809523808,\n",
              "          'news': 0.14285714285714285,\n",
              "          'stories': 0.011904761904761904,\n",
              "          'etc': 0.07142857142857142,\n",
              "          '.': 0.13095238095238096,\n",
              "          'second': 0.023809523809523808,\n",
              "          'query': 0.05952380952380952,\n",
              "          'sometimes': 0.023809523809523808,\n",
              "          'called': 0.05952380952380952,\n",
              "          'query-based': 0.011904761904761904,\n",
              "          'summarizes': 0.047619047619047616,\n",
              "          'objects': 0.03571428571428571,\n",
              "          'specific': 0.047619047619047616,\n",
              "          'able': 0.05952380952380952,\n",
              "          'summaries': 0.27380952380952384,\n",
              "          'machine-generated': 0.011904761904761904,\n",
              "          'user': 0.023809523809523808,\n",
              "          'needs': 0.011904761904761904,\n",
              "          'example': 0.23809523809523808,\n",
              "          'problem': 0.11904761904761904,\n",
              "          'attempts': 0.03571428571428571,\n",
              "          'automatically': 0.07142857142857142,\n",
              "          'produce': 0.13095238095238096,\n",
              "          'given': 0.07142857142857142,\n",
              "          'interested': 0.011904761904761904,\n",
              "          'generating': 0.07142857142857142,\n",
              "          'single': 0.05952380952380952,\n",
              "          'others': 0.047619047619047616,\n",
              "          'multiple': 0.08333333333333333,\n",
              "          'cluster': 0.03571428571428571,\n",
              "          'articles': 0.08333333333333333,\n",
              "          'topic': 0.07142857142857142,\n",
              "          'multi-document': 0.15476190476190477,\n",
              "          'related': 0.047619047619047616,\n",
              "          'application': 0.047619047619047616,\n",
              "          'summarizing': 0.05952380952380952,\n",
              "          'imagine': 0.03571428571428571,\n",
              "          'system': 0.19047619047619047,\n",
              "          'pulls': 0.011904761904761904,\n",
              "          'together': 0.05952380952380952,\n",
              "          'web': 0.03571428571428571,\n",
              "          'concisely': 0.011904761904761904,\n",
              "          'latest': 0.011904761904761904,\n",
              "          'another': 0.08333333333333333,\n",
              "          'consists': 0.011904761904761904,\n",
              "          'selecting': 0.03571428571428571,\n",
              "          'larger': 0.047619047619047616,\n",
              "          '5': 0.011904761904761904,\n",
              "          'context': 0.011904761904761904,\n",
              "          'useful': 0.047619047619047616,\n",
              "          'show': 0.047619047619047616,\n",
              "          'results': 0.11904761904761904,\n",
              "          'exploration': 0.011904761904761904,\n",
              "          'creates': 0.023809523809523808,\n",
              "          'trailer': 0.011904761904761904,\n",
              "          'long': 0.03571428571428571,\n",
              "          'applications': 0.03571428571428571,\n",
              "          'consumer': 0.011904761904761904,\n",
              "          'personal': 0.011904761904761904,\n",
              "          'want': 0.05952380952380952,\n",
              "          'skip': 0.011904761904761904,\n",
              "          'boring': 0.023809523809523808,\n",
              "          'repetitive': 0.011904761904761904,\n",
              "          'actions': 0.011904761904761904,\n",
              "          'similarly': 0.08333333333333333,\n",
              "          'surveillance': 0.011904761904761904,\n",
              "          'would': 0.17857142857142858,\n",
              "          'extract': 0.03571428571428571,\n",
              "          'suspicious': 0.011904761904761904,\n",
              "          'activity': 0.011904761904761904,\n",
              "          'ignoring': 0.011904761904761904,\n",
              "          'redundant': 0.023809523809523808,\n",
              "          'captured': 0.011904761904761904,\n",
              "          'high': 0.05952380952380952,\n",
              "          'level': 0.03571428571428571,\n",
              "          'algorithms': 0.09523809523809523,\n",
              "          'try': 0.023809523809523808,\n",
              "          'find': 0.03571428571428571,\n",
              "          'subsets': 0.011904761904761904,\n",
              "          'like': 0.14285714285714285,\n",
              "          'cover': 0.08333333333333333,\n",
              "          'core-set': 0.011904761904761904,\n",
              "          'model': 0.11904761904761904,\n",
              "          'notions': 0.023809523809523808,\n",
              "          'diversity': 0.11904761904761904,\n",
              "          'coverage': 0.07142857142857142,\n",
              "          'representativeness': 0.011904761904761904,\n",
              "          'based': 0.19047619047619047,\n",
              "          'additionally': 0.011904761904761904,\n",
              "          'naturally': 0.03571428571428571,\n",
              "          'problems': 0.08333333333333333,\n",
              "          'textrank': 0.15476190476190477,\n",
              "          'pagerank': 0.08333333333333333,\n",
              "          'submodular': 0.2976190476190476,\n",
              "          'function': 0.10714285714285714,\n",
              "          'determinantal': 0.03571428571428571,\n",
              "          'point': 0.03571428571428571,\n",
              "          'maximal': 0.023809523809523808,\n",
              "          'marginal': 0.023809523809523808,\n",
              "          'mmr': 0.023809523809523808,\n",
              "          'task': 0.09523809523809523,\n",
              "          'following': 0.047619047619047616,\n",
              "          'piece': 0.023809523809523808,\n",
              "          'journal': 0.023809523809523808,\n",
              "          'article': 0.03571428571428571,\n",
              "          'must': 0.03571428571428571,\n",
              "          'list': 0.023809523809523808,\n",
              "          'keywords': 0.023809523809523808,\n",
              "          'phrase': 0.047619047619047616,\n",
              "          'capture': 0.011904761904761904,\n",
              "          'primary': 0.011904761904761904,\n",
              "          'topics': 0.023809523809523808,\n",
              "          'discussed': 0.047619047619047616,\n",
              "          '6': 0.011904761904761904,\n",
              "          'case': 0.10714285714285714,\n",
              "          'research': 0.09523809523809523,\n",
              "          'many': 0.17857142857142858,\n",
              "          'authors': 0.03571428571428571,\n",
              "          'provide': 0.03571428571428571,\n",
              "          'manually': 0.023809523809523808,\n",
              "          'assigned': 0.023809523809523808,\n",
              "          'lacks': 0.011904761904761904,\n",
              "          'pre-existing': 0.011904761904761904,\n",
              "          'keyphrases': 0.38095238095238093,\n",
              "          'rarely': 0.011904761904761904,\n",
              "          'attached': 0.011904761904761904,\n",
              "          'number': 0.11904761904761904,\n",
              "          'consider': 0.011904761904761904,\n",
              "          'keyphrase': 0.23809523809523808,\n",
              "          'extractor': 0.023809523809523808,\n",
              "          'select': 0.03571428571428571,\n",
              "          'army': 0.011904761904761904,\n",
              "          'corps': 0.011904761904761904,\n",
              "          'engineers': 0.011904761904761904,\n",
              "          'president': 0.011904761904761904,\n",
              "          'bush': 0.011904761904761904,\n",
              "          'new': 0.047619047619047616,\n",
              "          'orleans': 0.011904761904761904,\n",
              "          'defective': 0.011904761904761904,\n",
              "          'flood-control': 0.011904761904761904,\n",
              "          'pumps': 0.011904761904761904,\n",
              "          'pulled': 0.011904761904761904,\n",
              "          'directly': 0.011904761904761904,\n",
              "          'contrast': 0.011904761904761904,\n",
              "          'somehow': 0.011904761904761904,\n",
              "          'internalize': 0.011904761904761904,\n",
              "          'generate': 0.023809523809523808,\n",
              "          'appear': 0.09523809523809523,\n",
              "          'closely': 0.011904761904761904,\n",
              "          'resemble': 0.011904761904761904,\n",
              "          'political': 0.011904761904761904,\n",
              "          'negligence': 0.011904761904761904,\n",
              "          'inadequate': 0.011904761904761904,\n",
              "          'protection': 0.011904761904761904,\n",
              "          'floods': 0.011904761904761904,\n",
              "          'requires': 0.023809523809523808,\n",
              "          'makes': 0.023809523809523808,\n",
              "          'computer': 0.023809523809523808,\n",
              "          'enable': 0.011904761904761904,\n",
              "          'browsing': 0.011904761904761904,\n",
              "          'providing': 0.011904761904761904,\n",
              "          'short': 0.023809523809523808,\n",
              "          'improve': 0.011904761904761904,\n",
              "          'retrieval': 0.023809523809523808,\n",
              "          'could': 0.07142857142857142,\n",
              "          'search': 0.03571428571428571,\n",
              "          'reliable': 0.011904761904761904,\n",
              "          'hits': 0.011904761904761904,\n",
              "          'full-text': 0.011904761904761904,\n",
              "          'employed': 0.011904761904761904,\n",
              "          'entries': 0.011904761904761904,\n",
              "          'large': 0.05952380952380952,\n",
              "          'corpus': 0.03571428571428571,\n",
              "          'different': 0.09523809523809523,\n",
              "          'literature': 0.011904761904761904,\n",
              "          'definition': 0.011904761904761904,\n",
              "          'words': 0.10714285714285714,\n",
              "          'phrases': 0.05952380952380952,\n",
              "          'keyword': 0.011904761904761904,\n",
              "          'highly': 0.07142857142857142,\n",
              "          'theme': 0.011904761904761904,\n",
              "          'beginning': 0.011904761904761904,\n",
              "          'work': 0.05952380952380952,\n",
              "          'turney': 0.08333333333333333,\n",
              "          '7': 0.011904761904761904,\n",
              "          'researchers': 0.023809523809523808,\n",
              "          'approached': 0.011904761904761904,\n",
              "          'supervised': 0.16666666666666666,\n",
              "          'learning': 0.14285714285714285,\n",
              "          'construct': 0.011904761904761904,\n",
              "          'unigram': 0.047619047619047616,\n",
              "          'bigram': 0.03571428571428571,\n",
              "          'trigram': 0.03571428571428571,\n",
              "          'found': 0.05952380952380952,\n",
              "          'though': 0.03571428571428571,\n",
              "          'units': 0.023809523809523808,\n",
              "          'possible': 0.023809523809523808,\n",
              "          'compute': 0.011904761904761904,\n",
              "          'various': 0.05952380952380952,\n",
              "          'features': 0.19047619047619047,\n",
              "          'describing': 0.011904761904761904,\n",
              "          'e.g': 0.03571428571428571,\n",
              "          'begin': 0.011904761904761904,\n",
              "          'upper-case': 0.011904761904761904,\n",
              "          'letter': 0.011904761904761904,\n",
              "          'assume': 0.011904761904761904,\n",
              "          'known': 0.11904761904761904,\n",
              "          'training': 0.14285714285714285,\n",
              "          'using': 0.2261904761904762,\n",
              "          'assign': 0.03571428571428571,\n",
              "          'positive': 0.023809523809523808,\n",
              "          'negative': 0.023809523809523808,\n",
              "          'labels': 0.023809523809523808,\n",
              "          'learn': 0.08333333333333333,\n",
              "          'classifier': 0.05952380952380952,\n",
              "          'discriminate': 0.023809523809523808,\n",
              "          'classifiers': 0.023809523809523808,\n",
              "          'make': 0.023809523809523808,\n",
              "          'binary': 0.03571428571428571,\n",
              "          'classification': 0.08333333333333333,\n",
              "          'test': 0.047619047619047616,\n",
              "          'probability': 0.023809523809523808,\n",
              "          'instance': 0.03571428571428571,\n",
              "          'rule': 0.023809523809523808,\n",
              "          'says': 0.011904761904761904,\n",
              "          'initial': 0.011904761904761904,\n",
              "          'capital': 0.011904761904761904,\n",
              "          'letters': 0.011904761904761904,\n",
              "          'likely': 0.05952380952380952,\n",
              "          'learner': 0.023809523809523808,\n",
              "          'manner': 0.023809523809523808,\n",
              "          'example-generation': 0.011904761904761904,\n",
              "          'strategy': 0.011904761904761904,\n",
              "          'run': 0.023809523809523808,\n",
              "          'determine': 0.047619047619047616,\n",
              "          'looking': 0.011904761904761904,\n",
              "          'decisions': 0.011904761904761904,\n",
              "          'probabilities': 0.047619047619047616,\n",
              "          'returned': 0.011904761904761904,\n",
              "          'learned': 0.011904761904761904,\n",
              "          'threshold': 0.047619047619047616,\n",
              "          'extractors': 0.011904761904761904,\n",
              "          'generally': 0.03571428571428571,\n",
              "          'evaluated': 0.023809523809523808,\n",
              "          'precision': 0.03571428571428571,\n",
              "          'recall': 0.05952380952380952,\n",
              "          'measures': 0.047619047619047616,\n",
              "          'proposed': 0.03571428571428571,\n",
              "          'actually': 0.011904761904761904,\n",
              "          'correct': 0.011904761904761904,\n",
              "          'true': 0.011904761904761904,\n",
              "          'f-score': 0.011904761904761904,\n",
              "          'harmonic': 0.011904761904761904,\n",
              "          'mean': 0.023809523809523808,\n",
              "          'f': 0.011904761904761904,\n",
              "          '2pr/': 0.011904761904761904,\n",
              "          'p': 0.011904761904761904,\n",
              "          'r': 0.011904761904761904,\n",
              "          'matches': 0.011904761904761904,\n",
              "          'checked': 0.011904761904761904,\n",
              "          'stemming': 0.011904761904761904,\n",
              "          'applying': 0.047619047619047616,\n",
              "          'normalization': 0.011904761904761904,\n",
              "          'designing': 0.011904761904761904,\n",
              "          'involves': 0.023809523809523808,\n",
              "          'deciding': 0.011904761904761904,\n",
              "          'several': 0.05952380952380952,\n",
              "          'choices': 0.011904761904761904,\n",
              "          'unsupervised': 0.08333333333333333,\n",
              "          'choice': 0.011904761904761904,\n",
              "          'exactly': 0.023809523809523808,\n",
              "          'unigrams': 0.14285714285714285,\n",
              "          'bigrams': 0.023809523809523808,\n",
              "          'trigrams': 0.023809523809523808,\n",
              "          'without': 0.023809523809523808,\n",
              "          'intervening': 0.011904761904761904,\n",
              "          'punctuation': 0.011904761904761904,\n",
              "          'removing': 0.011904761904761904,\n",
              "          'stopwords': 0.011904761904761904,\n",
              "          'hulth': 0.03571428571428571,\n",
              "          'showed': 0.011904761904761904,\n",
              "          'get': 0.03571428571428571,\n",
              "          'improvement': 0.011904761904761904,\n",
              "          'tokens': 0.011904761904761904,\n",
              "          'match': 0.023809523809523808,\n",
              "          'certain': 0.011904761904761904,\n",
              "          'patterns': 0.011904761904761904,\n",
              "          'part-of-speech': 0.011904761904761904,\n",
              "          'tags': 0.011904761904761904,\n",
              "          'ideally': 0.023809523809523808,\n",
              "          'mechanism': 0.011904761904761904,\n",
              "          'produces': 0.011904761904761904,\n",
              "          'labeled': 0.023809523809523808,\n",
              "          'candidates': 0.023809523809523808,\n",
              "          'never': 0.011904761904761904,\n",
              "          'containing': 0.011904761904761904,\n",
              "          'four': 0.03571428571428571,\n",
              "          'thus': 0.10714285714285714,\n",
              "          'suffer': 0.011904761904761904,\n",
              "          'lead': 0.011904761904761904,\n",
              "          'low': 0.011904761904761904,\n",
              "          'need': 0.13095238095238096,\n",
              "          'describe': 0.011904761904761904,\n",
              "          'enough': 0.011904761904761904,\n",
              "          'allow': 0.011904761904761904,\n",
              "          'algorithm': 0.21428571428571427,\n",
              "          'non-': 0.011904761904761904,\n",
              "          'typically': 0.05952380952380952,\n",
              "          'involve': 0.011904761904761904,\n",
              "          'term': 0.023809523809523808,\n",
              "          'frequencies': 0.011904761904761904,\n",
              "          'times': 0.03571428571428571,\n",
              "          'appears': 0.03571428571428571,\n",
              "          'current': 0.011904761904761904,\n",
              "          'length': 0.05952380952380952,\n",
              "          'relative': 0.011904761904761904,\n",
              "          'position': 0.05952380952380952,\n",
              "          'occurrence': 0.011904761904761904,\n",
              "          'boolean': 0.011904761904761904,\n",
              "          'syntactic': 0.011904761904761904,\n",
              "          'contains': 0.023809523809523808,\n",
              "          'caps': 0.011904761904761904,\n",
              "          'paper': 0.03571428571428571,\n",
              "          '12': 0.023809523809523808,\n",
              "          'uses': 0.047619047619047616,\n",
              "          'reduced': 0.011904761904761904,\n",
              "          'successful': 0.011904761904761904,\n",
              "          'kea': 0.011904761904761904,\n",
              "          'derived': 0.011904761904761904,\n",
              "          '’': 0.011904761904761904,\n",
              "          'seminal': 0.011904761904761904,\n",
              "          'end': 0.03571428571428571,\n",
              "          'return': 0.011904761904761904,\n",
              "          'limit': 0.023809523809523808,\n",
              "          'ensemble': 0.011904761904761904,\n",
              "          'i.e': 0.05952380952380952,\n",
              "          'votes': 0.011904761904761904,\n",
              "          'numeric': 0.011904761904761904,\n",
              "          'scores': 0.03571428571428571,\n",
              "          'thresholded': 0.011904761904761904,\n",
              "          'user-provided': 0.011904761904761904,\n",
              "          'technique': 0.03571428571428571,\n",
              "          'c4.5': 0.011904761904761904,\n",
              "          'decision': 0.023809523809523808,\n",
              "          'trees': 0.023809523809523808,\n",
              "          'implicitly': 0.011904761904761904,\n",
              "          'determines': 0.023809523809523808,\n",
              "          'appropriate': 0.011904761904761904,\n",
              "          'created': 0.05952380952380952,\n",
              "          'predict': 0.011904761904761904,\n",
              "          'virtually': 0.011904761904761904,\n",
              "          'naive': 0.023809523809523808,\n",
              "          'bayes': 0.023809523809523808,\n",
              "          'induction': 0.011904761904761904,\n",
              "          \"'s\": 0.05952380952380952,\n",
              "          'genex': 0.011904761904761904,\n",
              "          'genetic': 0.023809523809523808,\n",
              "          'parameters': 0.03571428571428571,\n",
              "          'domain-specific': 0.023809523809523808,\n",
              "          'follows': 0.011904761904761904,\n",
              "          'series': 0.011904761904761904,\n",
              "          'heuristics': 0.023809523809523808,\n",
              "          'identify': 0.03571428571428571,\n",
              "          'optimizes': 0.011904761904761904,\n",
              "          'respect': 0.023809523809523808,\n",
              "          'performance': 0.023809523809523808,\n",
              "          'nice': 0.023809523809523808,\n",
              "          'properties': 0.023809523809523808,\n",
              "          'interpretable': 0.011904761904761904,\n",
              "          'rules': 0.011904761904761904,\n",
              "          'characterize': 0.023809523809523808,\n",
              "          'require': 0.011904761904761904,\n",
              "          'amount': 0.011904761904761904,\n",
              "          'furthermore': 0.023809523809523808,\n",
              "          'tends': 0.011904761904761904,\n",
              "          'customize': 0.011904761904761904,\n",
              "          'resulting': 0.047619047619047616,\n",
              "          'necessarily': 0.023809523809523808,\n",
              "          'portable': 0.03571428571428571,\n",
              "          'demonstrate': 0.011904761904761904,\n",
              "          'angle': 0.011904761904761904,\n",
              "          'instead': 0.023809523809523808,\n",
              "          'trying': 0.023809523809523808,\n",
              "          'explicit': 0.011904761904761904,\n",
              "          '8': 0.011904761904761904,\n",
              "          'exploits': 0.011904761904761904,\n",
              "          'structure': 0.011904761904761904,\n",
              "          'central': 0.03571428571428571,\n",
              "          'selects': 0.011904761904761904,\n",
              "          'pages': 0.023809523809523808,\n",
              "          'notion': 0.023809523809523808,\n",
              "          'prestige': 0.011904761904761904,\n",
              "          'recommendation': 0.023809523809523808,\n",
              "          'social': 0.011904761904761904,\n",
              "          'networks': 0.011904761904761904,\n",
              "          'previous': 0.011904761904761904,\n",
              "          'rather': 0.011904761904761904,\n",
              "          'arbitrary': 0.03571428571428571,\n",
              "          'simply': 0.047619047619047616,\n",
              "          'intrinsic': 0.047619047619047616,\n",
              "          'easily': 0.03571428571428571,\n",
              "          'domains': 0.011904761904761904,\n",
              "          'languages': 0.011904761904761904,\n",
              "          'purpose': 0.023809523809523808,\n",
              "          'graph-based': 0.023809523809523808,\n",
              "          'ranking': 0.09523809523809523,\n",
              "          'nlp': 0.047619047619047616,\n",
              "          'essentially': 0.03571428571428571,\n",
              "          'runs': 0.011904761904761904,\n",
              "          'graph': 0.15476190476190477,\n",
              "          'specially': 0.011904761904761904,\n",
              "          'designed': 0.011904761904761904,\n",
              "          'particular': 0.023809523809523808,\n",
              "          'builds': 0.023809523809523808,\n",
              "          'vertices': 0.08333333333333333,\n",
              "          'edges': 0.08333333333333333,\n",
              "          'measure': 0.05952380952380952,\n",
              "          'lexical': 0.011904761904761904,\n",
              "          'similarity': 0.10714285714285714,\n",
              "          'unit': 0.011904761904761904,\n",
              "          'unlike': 0.011904761904761904,\n",
              "          'undirected': 0.011904761904761904,\n",
              "          'weighted': 0.011904761904761904,\n",
              "          'reflect': 0.011904761904761904,\n",
              "          'degree': 0.023809523809523808,\n",
              "          'constructed': 0.023809523809523808,\n",
              "          'form': 0.03571428571428571,\n",
              "          'stochastic': 0.011904761904761904,\n",
              "          'matrix': 0.023809523809523808,\n",
              "          'damping': 0.011904761904761904,\n",
              "          'factor': 0.023809523809523808,\n",
              "          'random': 0.09523809523809523,\n",
              "          'surfer': 0.011904761904761904,\n",
              "          'obtained': 0.023809523809523808,\n",
              "          'finding': 0.023809523809523808,\n",
              "          'eigenvector': 0.023809523809523808,\n",
              "          'corresponding': 0.023809523809523808,\n",
              "          'eigenvalue': 0.011904761904761904,\n",
              "          'stationary': 0.047619047619047616,\n",
              "          'distribution': 0.023809523809523808,\n",
              "          'walk': 0.05952380952380952,\n",
              "          'correspond': 0.011904761904761904,\n",
              "          'rank': 0.07142857142857142,\n",
              "          'potentially': 0.023809523809523808,\n",
              "          'something': 0.011904761904761904,\n",
              "          'similar': 0.15476190476190477,\n",
              "          'vertex': 0.03571428571428571,\n",
              "          'keep': 0.011904761904761904,\n",
              "          'small': 0.011904761904761904,\n",
              "          'decide': 0.011904761904761904,\n",
              "          'individual': 0.03571428571428571,\n",
              "          'step': 0.08333333333333333,\n",
              "          'merges': 0.011904761904761904,\n",
              "          'ranked': 0.05952380952380952,\n",
              "          'adjacent': 0.023809523809523808,\n",
              "          'multi-word': 0.011904761904761904,\n",
              "          'side': 0.011904761904761904,\n",
              "          'effect': 0.011904761904761904,\n",
              "          'allowing': 0.011904761904761904,\n",
              "          'us': 0.011904761904761904,\n",
              "          'advanced': 0.011904761904761904,\n",
              "          'ranks': 0.023809523809523808,\n",
              "          'look': 0.011904761904761904,\n",
              "          'see': 0.011904761904761904,\n",
              "          'consecutively': 0.011904761904761904,\n",
              "          'final': 0.047619047619047616,\n",
              "          'note': 0.047619047619047616,\n",
              "          'placed': 0.03571428571428571,\n",
              "          'filtered': 0.011904761904761904,\n",
              "          'part': 0.023809523809523808,\n",
              "          'speech': 0.011904761904761904,\n",
              "          'adjectives': 0.011904761904761904,\n",
              "          'nouns': 0.011904761904761904,\n",
              "          'best': 0.023809523809523808,\n",
              "          'linguistic': 0.011904761904761904,\n",
              "          'comes': 0.011904761904761904,\n",
              "          'play': 0.011904761904761904,\n",
              "          'word': 0.047619047619047616,\n",
              "          'co-occurrence': 0.03571428571428571,\n",
              "          'connected': 0.03571428571428571,\n",
              "          'edge': 0.023809523809523808,\n",
              "          'window': 0.011904761904761904,\n",
              "          'size': 0.023809523809523808,\n",
              "          'n': 0.03571428571428571,\n",
              "          'around': 0.023809523809523808,\n",
              "          '2–10': 0.011904761904761904,\n",
              "          'linked': 0.023809523809523808,\n",
              "          'string': 0.011904761904761904,\n",
              "          'cohesion': 0.011904761904761904,\n",
              "          'idea': 0.047619047619047616,\n",
              "          'near': 0.011904761904761904,\n",
              "          'meaningful': 0.011904761904761904,\n",
              "          'recommend': 0.023809523809523808,\n",
              "          'reader': 0.023809523809523808,\n",
              "          'since': 0.03571428571428571,\n",
              "          'method': 0.047619047619047616,\n",
              "          'limited': 0.011904761904761904,\n",
              "          'chosen': 0.011904761904761904,\n",
              "          'count': 0.03571428571428571,\n",
              "          'user-specified': 0.023809523809523808,\n",
              "          'fraction': 0.011904761904761904,\n",
              "          'total': 0.011904761904761904,\n",
              "          'top': 0.03571428571428571,\n",
              "          'vertices/unigrams': 0.011904761904761904,\n",
              "          'selected': 0.023809523809523808,\n",
              "          'post-': 0.011904761904761904,\n",
              "          'merge': 0.011904761904761904,\n",
              "          'instances': 0.03571428571428571,\n",
              "          'result': 0.03571428571428571,\n",
              "          'less': 0.011904761904761904,\n",
              "          'produced': 0.011904761904761904,\n",
              "          'roughly': 0.011904761904761904,\n",
              "          'proportional': 0.011904761904761904,\n",
              "          'initially': 0.011904761904761904,\n",
              "          'clear': 0.011904761904761904,\n",
              "          'think': 0.011904761904761904,\n",
              "          'throughout': 0.011904761904761904,\n",
              "          'co-occurring': 0.011904761904761904,\n",
              "          'neighbors': 0.023809523809523808,\n",
              "          'co-occur': 0.011904761904761904,\n",
              "          'un-supervised': 0.011904761904761904,\n",
              "          'semi-supervised': 0.011904761904761904,\n",
              "          'hub': 0.011904761904761904,\n",
              "          'connects': 0.011904761904761904,\n",
              "          'modifying': 0.011904761904761904,\n",
              "          'running': 0.011904761904761904,\n",
              "          'pagerank/textrank': 0.011904761904761904,\n",
              "          'places': 0.011904761904761904,\n",
              "          'importance': 0.07142857142857142,\n",
              "          'contribute': 0.011904761904761904,\n",
              "          'ends': 0.011904761904761904,\n",
              "          'along': 0.011904761904761904,\n",
              "          'probably': 0.023809523809523808,\n",
              "          'post-processing': 0.023809523809523808,\n",
              "          'contain': 0.03571428571428571,\n",
              "          'densely': 0.023809523809523808,\n",
              "          'regions': 0.011904761904761904,\n",
              "          'contexts': 0.011904761904761904,\n",
              "          'assigns': 0.011904761904761904,\n",
              "          'centers': 0.011904761904761904,\n",
              "          'clusters': 0.011904761904761904,\n",
              "          'getting': 0.023809523809523808,\n",
              "          'approach': 0.047619047619047616,\n",
              "          'considered': 0.047619047619047616,\n",
              "          'aims': 0.011904761904761904,\n",
              "          'essence': 0.011904761904761904,\n",
              "          'real': 0.011904761904761904,\n",
              "          'difference': 0.011904761904761904,\n",
              "          'dealing': 0.011904761904761904,\n",
              "          'units—whole': 0.011904761904761904,\n",
              "          'details': 0.011904761904761904,\n",
              "          'mention': 0.011904761904761904,\n",
              "          'common': 0.03571428571428571,\n",
              "          'so-called': 0.011904761904761904,\n",
              "          'rouge': 0.05952380952380952,\n",
              "          'recall-oriented': 0.023809523809523808,\n",
              "          'understudy': 0.023809523809523808,\n",
              "          'gisting': 0.023809523809523808,\n",
              "          'evaluation': 0.15476190476190477,\n",
              "          'recall-based': 0.023809523809523808,\n",
              "          'well': 0.011904761904761904,\n",
              "          'system-generated': 0.011904761904761904,\n",
              "          'covers': 0.011904761904761904,\n",
              "          'present': 0.023809523809523808,\n",
              "          'human-generated': 0.023809523809523808,\n",
              "          'references': 0.023809523809523808,\n",
              "          'encourage': 0.011904761904761904,\n",
              "          'computed': 0.023809523809523808,\n",
              "          '4-gram': 0.011904761904761904,\n",
              "          'matching': 0.011904761904761904,\n",
              "          'rouge-1': 0.03571428571428571,\n",
              "          'division': 0.011904761904761904,\n",
              "          'reference': 0.05952380952380952,\n",
              "          'averaged': 0.011904761904761904,\n",
              "          'overlap': 0.047619047619047616,\n",
              "          'concepts': 0.047619047619047616,\n",
              "          'coherent': 0.011904761904761904,\n",
              "          'flow': 0.011904761904761904,\n",
              "          'sensible': 0.011904761904761904,\n",
              "          'high-order': 0.011904761904761904,\n",
              "          'n-gram': 0.023809523809523808,\n",
              "          'judge': 0.011904761904761904,\n",
              "          'fluency': 0.011904761904761904,\n",
              "          'bleu': 0.023809523809523808,\n",
              "          'precision-': 0.011904761904761904,\n",
              "          'favor': 0.011904761904761904,\n",
              "          'accuracy': 0.011904761904761904,\n",
              "          'promising': 0.011904761904761904,\n",
              "          'line': 0.011904761904761904,\n",
              "          'adaptive': 0.03571428571428571,\n",
              "          'document/text': 0.023809523809523808,\n",
              "          '9': 0.011904761904761904,\n",
              "          'preliminary': 0.011904761904761904,\n",
              "          'recognition': 0.011904761904761904,\n",
              "          'genre': 0.023809523809523808,\n",
              "          'subsequent': 0.011904761904761904,\n",
              "          'optimized': 0.011904761904761904,\n",
              "          'perform': 0.023809523809523808,\n",
              "          '10': 0.011904761904761904,\n",
              "          'basically': 0.011904761904761904,\n",
              "          'good': 0.03571428571428571,\n",
              "          'sentence': 0.16666666666666666,\n",
              "          'main': 0.03571428571428571,\n",
              "          'difficulty': 0.011904761904761904,\n",
              "          'extracting': 0.011904761904761904,\n",
              "          'people': 0.011904761904761904,\n",
              "          'abstracts': 0.011904761904761904,\n",
              "          'existing': 0.03571428571428571,\n",
              "          'usually': 0.011904761904761904,\n",
              "          'sufficient': 0.011904761904761904,\n",
              "          'still': 0.03571428571428571,\n",
              "          'purposes': 0.011904761904761904,\n",
              "          'cares': 0.011904761904761904,\n",
              "          'duc': 0.023809523809523808,\n",
              "          '2001': 0.011904761904761904,\n",
              "          '2002': 0.023809523809523808,\n",
              "          'workshops': 0.011904761904761904,\n",
              "          'tno': 0.011904761904761904,\n",
              "          'developed': 0.047619047619047616,\n",
              "          'hybrid': 0.011904761904761904,\n",
              "          'statistical': 0.023809523809523808,\n",
              "          'models': 0.08333333333333333,\n",
              "          'modeling': 0.023809523809523808,\n",
              "          'salience': 0.011904761904761904,\n",
              "          'although': 0.023809523809523808,\n",
              "          'exhibited': 0.011904761904761904,\n",
              "          'wanted': 0.011904761904761904,\n",
              "          'explore': 0.011904761904761904,\n",
              "          'effectiveness': 0.011904761904761904,\n",
              "          'maximum': 0.023809523809523808,\n",
              "          'entropy': 0.023809523809523808,\n",
              "          'meeting': 0.011904761904761904,\n",
              "          'robust': 0.011904761904761904,\n",
              "          'feature': 0.011904761904761904,\n",
              "          'dependencies': 0.011904761904761904,\n",
              "          'successfully': 0.023809523809523808,\n",
              "          'broadcast': 0.011904761904761904,\n",
              "          'quite': 0.011904761904761904,\n",
              "          'spirit': 0.011904761904761904,\n",
              "          'gets': 0.011904761904761904,\n",
              "          'issue': 0.023809523809523808,\n",
              "          'costly': 0.011904761904761904,\n",
              "          'centroid': 0.023809523809523808,\n",
              "          'vector': 0.011904761904761904,\n",
              "          'regard': 0.023809523809523808,\n",
              "          'principled': 0.011904761904761904,\n",
              "          'estimate': 0.011904761904761904,\n",
              "          'walks': 0.03571428571428571,\n",
              "          'centrality': 0.023809523809523808,\n",
              "          'lexrank': 0.11904761904761904,\n",
              "          '11': 0.011904761904761904,\n",
              "          'identical': 0.011904761904761904,\n",
              "          'groups': 0.023809523809523808,\n",
              "          'time': 0.03571428571428571,\n",
              "          'focused': 0.011904761904761904,\n",
              "          'creating': 0.011904761904761904,\n",
              "          'cosine': 0.023809523809523808,\n",
              "          'tf-idf': 0.011904761904761904,\n",
              "          'vectors': 0.011904761904761904,\n",
              "          'normalized': 0.011904761904761904,\n",
              "          'lengths': 0.011904761904761904,\n",
              "          'explored': 0.011904761904761904,\n",
              "          'unweighted': 0.011904761904761904,\n",
              "          'values': 0.011904761904761904,\n",
              "          'experimented': 0.011904761904761904,\n",
              "          'weights': 0.03571428571428571,\n",
              "          'equal': 0.011904761904761904,\n",
              "          'score': 0.03571428571428571,\n",
              "          'continuous': 0.011904761904761904,\n",
              "          'formed': 0.011904761904761904,\n",
              "          'combining': 0.011904761904761904,\n",
              "          'cutoff': 0.011904761904761904,\n",
              "          'worth': 0.011904761904761904,\n",
              "          'noting': 0.011904761904761904,\n",
              "          'described': 0.023809523809523808,\n",
              "          'mead': 0.011904761904761904,\n",
              "          'combines': 0.011904761904761904,\n",
              "          'linear': 0.011904761904761904,\n",
              "          'combination': 0.011904761904761904,\n",
              "          'either': 0.011904761904761904,\n",
              "          'tuned': 0.011904761904761904,\n",
              "          'additional': 0.011904761904761904,\n",
              "          'absolutely': 0.011904761904761904,\n",
              "          'necessary': 0.011904761904761904,\n",
              "          'distinction': 0.011904761904761904,\n",
              "          'remains': 0.023809523809523808,\n",
              "          'cases—only': 0.011904761904761904,\n",
              "          'choose': 0.011904761904761904,\n",
              "          'grown': 0.011904761904761904,\n",
              "          'greater': 0.011904761904761904,\n",
              "          'risk': 0.011904761904761904,\n",
              "          'duplicate': 0.011904761904761904,\n",
              "          'place': 0.011904761904761904,\n",
              "          'event': 0.011904761904761904,\n",
              "          'distinct': 0.011904761904761904,\n",
              "          'ideas': 0.023809523809523808,\n",
              "          'address': 0.011904761904761904,\n",
              "          'applies': 0.023809523809523808,\n",
              "          'heuristic': 0.023809523809523808,\n",
              "          'adding': 0.011904761904761904,\n",
              "          'order': 0.023809523809523808,\n",
              "          'discards': 0.011904761904761904,\n",
              "          'ones': 0.023809523809523808,\n",
              "          'already': 0.011904761904761904,\n",
              "          'cross-sentence': 0.011904761904761904,\n",
              "          'subsumption': 0.011904761904761904,\n",
              "          'csis': 0.023809523809523808,\n",
              "          'great': 0.011904761904761904,\n",
              "          'stems': 0.011904761904761904,\n",
              "          'recommending': 0.011904761904761904,\n",
              "          'turn': 0.011904761904761904,\n",
              "          'intuitive': 0.011904761904761904,\n",
              "          'sense': 0.011904761904761904,\n",
              "          'allows': 0.023809523809523808,\n",
              "          'domain-independent': 0.011904761904761904,\n",
              "          'indicating': 0.011904761904761904,\n",
              "          'vary': 0.011904761904761904,\n",
              "          'considerably': 0.011904761904761904,\n",
              "          'biomedical': 0.011904761904761904,\n",
              "          '-based': 0.011904761904761904,\n",
              "          'procedure': 0.023809523809523808,\n",
              "          'texts': 0.023809523809523808,\n",
              "          'written': 0.011904761904761904,\n",
              "          'report': 0.011904761904761904,\n",
              "          'users': 0.011904761904761904,\n",
              "          'professional': 0.011904761904761904,\n",
              "          'consumers': 0.011904761904761904,\n",
              "          'quickly': 0.011904761904761904,\n",
              "          'familiarize': 0.011904761904761904,\n",
              "          'contained': 0.011904761904761904,\n",
              "          'complementing': 0.011904761904761904,\n",
              "          'aggregators': 0.011904761904761904,\n",
              "          'performing': 0.011904761904761904,\n",
              "          'next': 0.011904761904761904,\n",
              "          'road': 0.011904761904761904,\n",
              "          'coping': 0.011904761904761904,\n",
              "          'overload': 0.011904761904761904,\n",
              "          'done': 0.023809523809523808,\n",
              "          'response': 0.011904761904761904,\n",
              "          'question': 0.011904761904761904,\n",
              "          'reports': 0.011904761904761904,\n",
              "          'concise': 0.011904761904761904,\n",
              "          'comprehensive': 0.023809523809523808,\n",
              "          'opinions': 0.011904761904761904,\n",
              "          'put': 0.011904761904761904,\n",
              "          'outlined': 0.011904761904761904,\n",
              "          'every': 0.011904761904761904,\n",
              "          'perspectives': 0.011904761904761904,\n",
              "          'goal': 0.011904761904761904,\n",
              "          'brief': 0.011904761904761904,\n",
              "          'simplify': 0.011904761904761904,\n",
              "          'cut': 0.011904761904761904,\n",
              "          'pointing': 0.011904761904761904,\n",
              "          'required': 0.023809523809523808,\n",
              "          'hence': 0.023809523809523808,\n",
              "          'limiting': 0.011904761904761904,\n",
              "          'accessing': 0.011904761904761904,\n",
              "          'files': 0.011904761904761904,\n",
              "          'refinement': 0.011904761904761904,\n",
              "          'sources': 0.023809523809523808,\n",
              "          'algorithmically': 0.011904761904761904,\n",
              "          'editorial': 0.011904761904761904,\n",
              "          'touch': 0.011904761904761904,\n",
              "          'subjective': 0.011904761904761904,\n",
              "          'making': 0.023809523809523808,\n",
              "          'completely': 0.011904761904761904,\n",
              "          'unbiased': 0.011904761904761904,\n",
              "          'dubious': 0.011904761904761904,\n",
              "          '–': 0.011904761904761904,\n",
              "          'discuss': 0.011904761904761904,\n",
              "          'faces': 0.011904761904761904,\n",
              "          'potential': 0.011904761904761904,\n",
              "          'redundancy': 0.03571428571428571,\n",
              "          'diverse': 0.011904761904761904,\n",
              "          'differ': 0.011904761904761904,\n",
              "          'deals': 0.011904761904761904,\n",
              "          'stage': 0.011904761904761904,\n",
              "          '13': 0.011904761904761904,\n",
              "          'eliminate': 0.011904761904761904,\n",
              "          'page/lex/textrank': 0.011904761904761904,\n",
              "          'handles': 0.011904761904761904,\n",
              "          'unified': 0.011904761904761904,\n",
              "          'mathematical': 0.011904761904761904,\n",
              "          'framework': 0.011904761904761904,\n",
              "          'absorbing': 0.047619047619047616,\n",
              "          'markov': 0.011904761904761904,\n",
              "          'chain': 0.011904761904761904,\n",
              "          'standard': 0.011904761904761904,\n",
              "          'except': 0.011904761904761904,\n",
              "          'states': 0.023809523809523808,\n",
              "          'act': 0.011904761904761904,\n",
              "          'black': 0.011904761904761904,\n",
              "          'holes': 0.011904761904761904,\n",
              "          'cause': 0.011904761904761904,\n",
              "          'abruptly': 0.011904761904761904,\n",
              "          'state': 0.03571428571428571,\n",
              "          'grasshopper': 0.03571428571428571,\n",
              "          '14': 0.011904761904761904,\n",
              "          'explicitly': 0.011904761904761904,\n",
              "          'promoting': 0.011904761904761904,\n",
              "          'incorporates': 0.011904761904761904,\n",
              "          'prior': 0.011904761904761904,\n",
              "          'art': 0.023809523809523808,\n",
              "          'mixtures': 0.023809523809523808,\n",
              "          'functions': 0.16666666666666666,\n",
              "          'achieved': 0.03571428571428571,\n",
              "          'corpora': 0.011904761904761904,\n",
              "          '04': 0.011904761904761904,\n",
              "          '07': 0.011904761904761904,\n",
              "          '15': 0.011904761904761904,\n",
              "          'processes': 0.011904761904761904,\n",
              "          'duc-04': 0.023809523809523808,\n",
              "          '16': 0.011904761904761904,\n",
              "          'multi-lingual': 0.011904761904761904,\n",
              "          'avoids': 0.011904761904761904,\n",
              "          'works': 0.023809523809523808,\n",
              "          'simplifying': 0.011904761904761904,\n",
              "          'ideograms': 0.03571428571428571,\n",
              "          'represent': 0.023809523809523808,\n",
              "          'meaning': 0.023809523809523808,\n",
              "          'evaluates': 0.011904761904761904,\n",
              "          'qualitatively': 0.011904761904761904,\n",
              "          'comparing': 0.011904761904761904,\n",
              "          'shape': 0.011904761904761904,\n",
              "          'said': 0.011904761904761904,\n",
              "          'recently': 0.023809523809523808,\n",
              "          'tool': 0.023809523809523808,\n",
              "          'frequency': 0.023809523809523808,\n",
              "          'preprocessing': 0.011904761904761904,\n",
              "          'kind': 0.011904761904761904,\n",
              "          'user-supplied': 0.011904761904761904,\n",
              "          'equivalence': 0.011904761904761904,\n",
              "          'equivalent': 0.011904761904761904,\n",
              "          'desired': 0.011904761904761904,\n",
              "          'emerged': 0.011904761904761904,\n",
              "          'powerful': 0.023809523809523808,\n",
              "          'moreover': 0.047619047619047616,\n",
              "          'combinatorial': 0.011904761904761904,\n",
              "          'optimization': 0.07142857142857142,\n",
              "          'occur': 0.011904761904761904,\n",
              "          ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmWhowBD8Xre",
        "outputId": "a15e913b-cdbd-4154-8530-a9210fa45906"
      },
      "source": [
        "score_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words.': 3.7142857142857135,\n",
              " '\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.': 2.964285714285714,\n",
              " '\"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\".': 1.4642857142857142,\n",
              " '\"Summarizing Conceptual Graphs for Automatic Summarization Task\".': 1.4642857142857142,\n",
              " '(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.)': 2.273809523809523,\n",
              " ', Conceptual Structures for STEM Research and Education.': 0.13095238095238096,\n",
              " ', The GRASSHOPPER algorithm\\n• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013).': 0.45238095238095244,\n",
              " '222–235.': 0.14285714285714285,\n",
              " '245–253.': 0.14285714285714285,\n",
              " '650. pp.': 0.16666666666666669,\n",
              " '7735. pp.': 0.16666666666666669,\n",
              " 'A high level of overlap should indicate a high level of shared concepts between the two summaries.': 0.8333333333333334,\n",
              " 'A more principled way to estimate sentence importance is using random walks and eigenvector centrality.': 0.9523809523809523,\n",
              " 'A post- processing step is then applied to merge adjacent instances of these T unigrams.': 0.5595238095238095,\n",
              " 'A promising line in document summarization is adaptive document/text summarization.': 2.547619047619048,\n",
              " 'A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters.': 0.7023809523809523,\n",
              " 'A related application is summarizing news articles.': 0.5119047619047619,\n",
              " 'A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.': 1.857142857142857,\n",
              " 'A word that appears multiple times throughout a text may have many different co-occurring neighbors.': 1.25,\n",
              " 'Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction.': 1.6428571428571428,\n",
              " 'Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.': 1.047619047619048,\n",
              " 'Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express.': 1.5833333333333333,\n",
              " 'Advances in Intelligent Systems and Computing.': 0.13095238095238096,\n",
              " 'After training a learner, we can select keyphrases for test documents in the following manner.': 1.011904761904762,\n",
              " 'All these important models encouraging coverage, diversity and information are all submodular.': 1.0595238095238095,\n",
              " 'Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies.': 1.9166666666666665,\n",
              " 'Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.': 2.1547619047619047,\n",
              " 'An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.': 3.5000000000000004,\n",
              " 'Anaphor resolution remains another problem yet to be fully solved.': 0.40476190476190477,\n",
              " 'Another example of a submodular optimization problem is using a determinantal point process to model diversity.': 1.476190476190476,\n",
              " 'Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization.': 3.1785714285714284,\n",
              " 'Another keyphrase extraction algorithm is TextRank.': 0.8452380952380952,\n",
              " 'Approaches aimed at higher summarization quality rely on combined software and human effort.': 1.452380952380952,\n",
              " 'As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.': 1.5238095238095237,\n",
              " 'At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.': 3.0476190476190483,\n",
              " 'Automatic Keyphrases Extraction.': 0.13095238095238096,\n",
              " 'Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.': 1.1785714285714284,\n",
              " 'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.': 2.785714285714285,\n",
              " 'Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.': 1.8452380952380951,\n",
              " 'Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.': 2.3571428571428568,\n",
              " 'Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.': 2.595238095238095,\n",
              " 'Beginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem.': 1.4404761904761905,\n",
              " 'By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization.': 1.3928571428571428,\n",
              " 'Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation.': 0.13095238095238096,\n",
              " 'Conceptual Structures for STEM Research and Education.': 0.13095238095238096,\n",
              " 'Consider the example text from a news article:\\n\\nA keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases.': 7.2738095238095255,\n",
              " 'Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.': 0.9285714285714286,\n",
              " 'Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too).': 1.226190476190476,\n",
              " 'During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.': 2.4285714285714284,\n",
              " 'Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary.': 1.702380952380952,\n",
              " 'Edges are based on some measure of semantic or lexical similarity between the text unit vertices.': 1.1904761904761905,\n",
              " 'Edges are created based on word co-occurrence in this application of TextRank.': 0.5119047619047619,\n",
              " 'Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases.': 1.6428571428571428,\n",
              " 'Essentially, it runs PageRank on a graph specially designed for a particular NLP task.': 0.44047619047619047,\n",
              " 'Evaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual.': 0.3571428571428571,\n",
              " 'Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.': 3.7499999999999996,\n",
              " 'Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.': 1.5714285714285712,\n",
              " 'Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.': 1.869047619047619,\n",
              " 'First summarizes that perform adaptive summarization have been created.': 1.2976190476190477,\n",
              " 'For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.': 1.5595238095238093,\n",
              " 'For example, a simple greedy algorithm admits a constant factor guarantee.': 0.6904761904761904,\n",
              " 'For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.': 2.5238095238095224,\n",
              " 'For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.': 8.023809523809522,\n",
              " 'For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.': 1.273809523809524,\n",
              " 'For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences.': 8.964285714285715,\n",
              " 'For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.': 3.3095238095238093,\n",
              " 'For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.': 1.5595238095238095,\n",
              " 'For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.': 2.0833333333333335,\n",
              " 'For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.': 2.607142857142857,\n",
              " 'For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.': 1.5,\n",
              " 'For keyphrase extraction, it builds a graph using some set of text units as vertices.': 1.9166666666666665,\n",
              " 'For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.': 2.94047619047619,\n",
              " 'Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g.': 1.1547619047619049,\n",
              " \"Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\": 1.3095238095238095,\n",
              " 'Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below).': 2.392857142857143,\n",
              " 'Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.': 2.7261904761904754,\n",
              " 'Here, content is extracted from the original data, but the extracted content is not modified in any way.': 0.8809523809523808,\n",
              " 'High-order n-gram ROUGE measures try to judge fluency to some degree.': 0.27380952380952384,\n",
              " 'However, generating too many examples can also lead to low precision.': 0.8452380952380951,\n",
              " 'However, the unsupervised \"recommendation\"-based approach applies to any domain.': 1.7857142857142856,\n",
              " 'However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases.': 1.4047619047619047,\n",
              " 'However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary.': 1.4166666666666665,\n",
              " 'Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags.': 0.4642857142857143,\n",
              " 'Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.': 1.0833333333333333,\n",
              " 'Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper.': 0.8095238095238093,\n",
              " 'Human judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult.': 2.583333333333333,\n",
              " 'ISBN .': 0.3928571428571429,\n",
              " 'Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case.': 1.1071428571428572,\n",
              " 'Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another).': 4.011904761904762,\n",
              " 'If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\".': 3.797619047619047,\n",
              " 'If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\".': 3.6309523809523805,\n",
              " 'If probabilities are given, a threshold is used to select the keyphrases.': 0.988095238095238,\n",
              " 'If there are multiple references, the ROUGE-1 scores are averaged.': 0.2857142857142857,\n",
              " 'Image collection summarization is another application example of automatic summarization.': 2.7857142857142856,\n",
              " 'Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.': 1.452380952380952,\n",
              " 'Imagine you have a cluster of news articles on a particular event, and you want to produce one summary.': 1.2380952380952381,\n",
              " 'Improving diversity in ranking using absorbing random walks .': 0.7499999999999999,\n",
              " 'In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.': 1.0476190476190474,\n",
              " 'In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text).': 1.0476190476190477,\n",
              " 'In addition to text, images and videos can also be summarized.': 1.1309523809523812,\n",
              " 'In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries.': 1.964285714285714,\n",
              " 'In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.': 0.857142857142857,\n",
              " 'In both algorithms, the sentences are ranked by applying PageRank to the resulting graph.': 0.9285714285714285,\n",
              " 'In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".': 5.130952380952381,\n",
              " 'In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts.': 0.7619047619047619,\n",
              " 'In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.': 2.1071428571428568,\n",
              " \"In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.\": 1.880952380952381,\n",
              " 'In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number.': 1.714285714285714,\n",
              " 'In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".': 4.107142857142857,\n",
              " 'In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF).': 0.40476190476190477,\n",
              " 'In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.': 1.1666666666666665,\n",
              " \"In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.\": 2.2738095238095233,\n",
              " 'Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.': 4.416666666666667,\n",
              " 'Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.': 2.94047619047619,\n",
              " 'Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries.': 0.5238095238095238,\n",
              " 'It approaches the problem from a different angle.': 0.41666666666666663,\n",
              " 'It consists in selecting a representative set of images from a larger set of images.': 0.9166666666666666,\n",
              " 'It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries.': 0.9880952380952381,\n",
              " 'It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases.': 1.130952380952381,\n",
              " 'It is recall-based to encourage systems to include all the important topics in the text.': 1.119047619047619,\n",
              " 'It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.': 3.9523809523809517,\n",
              " 'It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times.': 0.630952380952381,\n",
              " 'Keyphrase extractors are generally evaluated using precision and recall.': 0.5238095238095238,\n",
              " 'Keyphrases have many applications.': 0.34523809523809523,\n",
              " 'Lecture Notes in Computer Science.': 0.13095238095238096,\n",
              " 'LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results.': 1.7619047619047616,\n",
              " 'LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.': 1.880952380952381,\n",
              " 'Like keyphrase extraction, document summarization aims to identify the essence of a text.': 2.5714285714285716,\n",
              " 'Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents.': 1.4761904761904763,\n",
              " 'Many documents with known keyphrases are needed.': 0.8571428571428572,\n",
              " 'Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.': 1.6785714285714284,\n",
              " 'Maximum entropy has also been applied successfully for summarization in the broadcast news domain.': 1.8095238095238093,\n",
              " 'Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization.': 0.976190476190476,\n",
              " 'Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular.': 1.2261904761904763,\n",
              " 'Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.': 0.5833333333333334,\n",
              " 'Much effort has to be done in order to have corpus of texts and their corresponding summaries.': 0.5595238095238095,\n",
              " 'Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.': 2.047619047619047,\n",
              " 'Multi-document summarization may also be done in response to a question.': 1.5,\n",
              " 'N is typically around 2–10.': 0.2261904761904762,\n",
              " 'Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy.': 0.9166666666666666,\n",
              " \"Note that overlap metrics like this are unable to provide any feedback on a summary's coherence.\": 0.9047619047619048,\n",
              " 'Note that the unigrams placed in the graph can be filtered by part of speech.': 0.511904761904762,\n",
              " 'Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams.': 1.2142857142857142,\n",
              " 'Once examples and features are created, we need a way to learn to predict keyphrases.': 1.2738095238095237,\n",
              " 'Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).': 3.011904761904761,\n",
              " 'One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain.': 1.5476190476190474,\n",
              " \"One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]).\": 1.9285714285714284,\n",
              " 'One way to think about it is the following.': 0.34523809523809523,\n",
              " 'Other issues are those concerning coherence and coverage.': 0.27380952380952384,\n",
              " 'Pattern-based summarization was the most powerful option for multi-document summarization found by 2016.': 2.3809523809523805,\n",
              " 'Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc.': 1.0238095238095237,\n",
              " 'Precision measures how many of the proposed keyphrases are actually correct.': 0.7976190476190474,\n",
              " 'Query based summarization techniques, additionally model for relevance of the summary with the query.': 2.0595238095238093,\n",
              " 'Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching.': 0.3214285714285714,\n",
              " 'Recall measures how many of the true keyphrases your system proposed.': 0.9761904761904762,\n",
              " 'Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks.': 3.0714285714285707,\n",
              " 'Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.': 2.202380952380952,\n",
              " 'Research increased significantly in 2015.': 0.16666666666666669,\n",
              " 'Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents.': 1.3452380952380951,\n",
              " 'Running PageRank/TextRank on the graph is likely to rank \"learning\" highly.': 1.9642857142857142,\n",
              " 'SCU in the Pyramid Method).': 0.13095238095238096,\n",
              " 'Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies.': 1.0119047619047619,\n",
              " 'Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.': 2.5714285714285716,\n",
              " 'Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\".': 5.452380952380952,\n",
              " 'Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.': 0.9523809523809522,\n",
              " 'Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization.': 0.8452380952380952,\n",
              " 'Similarly, the facility location problem is a special case of submodular functions.': 0.9285714285714285,\n",
              " 'Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions.': 2.333333333333333,\n",
              " 'Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases.': 1.3452380952380951,\n",
              " 'Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase.': 0.9285714285714286,\n",
              " 'Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.': 2.1904761904761902,\n",
              " 'Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.': 3.8214285714285707,\n",
              " 'Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).': 2.3095238095238098,\n",
              " 'Submodular Functions have also successfully been used for summarizing machine learning datasets.': 1.023809523809524,\n",
              " 'Submodular functions have achieved state-of-the-art for almost all summarization problems.': 1.4523809523809523,\n",
              " 'Submodular functions naturally model notions of coverage, information, representation and diversity.': 0.9047619047619048,\n",
              " 'Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.': 2.297619047619047,\n",
              " 'Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.': 1.761904761904762,\n",
              " 'Term frequency–inverse document frequency had been used by 2016.': 0.7738095238095238,\n",
              " 'Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.': 4.928571428571429,\n",
              " 'TextRank is a general purpose graph-based ranking algorithm for NLP.': 0.5476190476190476,\n",
              " 'TextRank uses continuous similarity scores as weights.': 0.36904761904761907,\n",
              " 'The Facility Location function also naturally models coverage and diversity.': 0.8214285714285714,\n",
              " 'The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score.': 1.4166666666666667,\n",
              " 'The Theory and Practice of Discourse Parsing and Summarization.': 0.13095238095238096,\n",
              " 'The Turney paper used about 12 such features.': 0.6547619047619048,\n",
              " 'The Use of Topic Segmentation for Automatic Summarization.': 0.13095238095238096,\n",
              " 'The algorithm is called GRASSHOPPER.': 0.40476190476190477,\n",
              " 'The authors found that adjectives and nouns were the best to include.': 0.3571428571428571,\n",
              " 'The edges between sentences are based on some form of semantic similarity or content overlap.': 1.1666666666666665,\n",
              " 'The extractor follows a series of heuristics to identify keyphrases.': 0.6190476190476191,\n",
              " 'The first choice is exactly how to generate examples.': 0.4285714285714286,\n",
              " 'The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).': 2.5595238095238093,\n",
              " 'The first publication in the area dates back to 1958 (Lun), starting with a statistical technique.': 0.36904761904761907,\n",
              " 'The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.': 1.0476190476190477,\n",
              " 'The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.': 2.0595238095238098,\n",
              " 'The importance of this sentence also stems from the importance of the sentences \"recommending\" it.': 2.464285714285714,\n",
              " 'The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".': 6.809523809523811,\n",
              " 'The method used is called Cross-Sentence Information Subsumption (CSIS).': 0.511904761904762,\n",
              " 'The methods are domain-independent and easily portable.': 0.3928571428571428,\n",
              " 'The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.': 0.6190476190476191,\n",
              " 'The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.': 1.3571428571428572,\n",
              " 'The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.': 2.559523809523809,\n",
              " 'The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.': 2.476190476190476,\n",
              " 'The set cover function attempts to find a subset of objects which cover a given set of concepts.': 1.107142857142857,\n",
              " 'The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions.': 2.2857142857142856,\n",
              " 'The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience.': 1.214285714285714,\n",
              " 'The task is the following.': 0.27380952380952384,\n",
              " 'The task remains the same in both cases—only the number of sentences to choose from has grown.': 0.7976190476190474,\n",
              " 'The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph.': 0.8452380952380953,\n",
              " 'The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ).': 0.5238095238095238,\n",
              " 'The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.': 2.7619047619047623,\n",
              " 'The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.': 2.583333333333333,\n",
              " 'The vertices should correspond to what we want to rank.': 0.35714285714285715,\n",
              " 'Then the sentences can be ranked with regard to their similarity to this centroid sentence.': 0.9047619047619047,\n",
              " 'Then the top T vertices/unigrams are selected based on their stationary probabilities.': 0.488095238095238,\n",
              " 'Then we learn a classifier that can discriminate between positive and negative examples as a function of the features.': 0.7738095238095237,\n",
              " 'There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.': 2.464285714285714,\n",
              " 'There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.': 3.928571428571428,\n",
              " 'These algorithms model notions like diversity, coverage, information and representativeness of the summary.': 1.3095238095238093,\n",
              " 'These are pulled directly from the text.': 0.7023809523809524,\n",
              " 'These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.': 4.07142857142857,\n",
              " 'These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.': 0.5476190476190477,\n",
              " 'These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader.': 2.928571428571428,\n",
              " 'They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.': 3.0714285714285707,\n",
              " 'This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions.': 0.988095238095238,\n",
              " 'This approach has also been used in document summarization, considered below.': 2.107142857142857,\n",
              " 'This has a nice side effect of allowing us to produce keyphrases of arbitrary length.': 0.8095238095238094,\n",
              " 'This is a hard and expensive task.': 0.25,\n",
              " 'This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references.': 1.6071428571428568,\n",
              " 'This is also called the core-set.': 0.4761904761904763,\n",
              " 'This is an instance of set cover.': 0.47619047619047616,\n",
              " 'This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient.': 1.2023809523809523,\n",
              " 'This is similar to densely connected Web pages getting ranked highly by PageRank.': 0.5238095238095238,\n",
              " 'This is the technique used by Turney with C4.5 decision trees.': 0.48809523809523814,\n",
              " 'This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text.': 1.0,\n",
              " 'This problem is called multi-document summarization.': 1.4642857142857142,\n",
              " 'This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).': 2.511904761904762,\n",
              " 'This was a breakthrough result establishing submodular functions as the right models for summarization problems.': 1.8452380952380951,\n",
              " 'Thus the algorithm is easily portable to new domains and languages.': 0.48809523809523814,\n",
              " 'Thus, \"natural\" and \"language\" might be linked in a text about NLP.': 3.6071428571428563,\n",
              " 'Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance.': 1.2142857142857144,\n",
              " 'Thus, recall may suffer.': 0.25,\n",
              " 'Thus, some linguistic knowledge comes into play in this step.': 0.2976190476190476,\n",
              " 'Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words.': 3.3333333333333335,\n",
              " 'Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.': 2.666666666666666,\n",
              " 'To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.': 2.261904761904762,\n",
              " 'Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization.': 2.0595238095238098,\n",
              " 'Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords.': 0.7380952380952379,\n",
              " 'Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text.': 1.2738095238095237,\n",
              " 'Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.': 2.238095238095238,\n",
              " 'Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity.': 0.44047619047619047,\n",
              " 'Unsupervised keyphrase extraction removes the need for training data.': 1.0,\n",
              " 'Using the known keyphrases, we can assign positive or negative labels to the examples.': 0.8690476190476191,\n",
              " 'Video summarization is a related domain, where the system automatically creates a trailer of a long video.': 1.7142857142857142,\n",
              " 'Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction.': 1.0833333333333335,\n",
              " 'We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.': 2.1547619047619047,\n",
              " 'We apply the same example-generation strategy to the test documents, then run each example through the learner.': 0.7142857142857143,\n",
              " 'We assume there are known keyphrases available for a set of training documents.': 1.2380952380952381,\n",
              " 'We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model.': 0.8928571428571428,\n",
              " 'We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?).': 0.726190476190476,\n",
              " \"While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).\": 1.9642857142857142,\n",
              " 'While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization.': 2.154761904761905,\n",
              " 'While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.': 1.9047619047619047,\n",
              " 'While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.': 2.369047619047618,\n",
              " 'With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document.': 0.9642857142857142,\n",
              " 'You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.': 1.7976190476190477,\n",
              " '[10]\\n\\nSupervised text summarization is very much like supervised keyphrase extraction.': 2.5357142857142856,\n",
              " '[12] [4]\\n\\nMulti-document summarization creates information reports that are both concise and comprehensive.': 1.452380952380952,\n",
              " '[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).': 2.130952380952381,\n",
              " '[15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.': 1.380952380952381,\n",
              " '[16]\\n\\nA new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.': 3.8095238095238084,\n",
              " '[17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.': 1.75,\n",
              " '[22]\\n• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts.': 2.0238095238095237,\n",
              " '[24] The name is reference to TL;DR − Internet slang for \"too long; didn\\'t read\".': 1.6666666666666665,\n",
              " '[25][26]\\n\\nThe most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.': 1.3095238095238098,\n",
              " '[28]\\n\\nAn intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.': 3.083333333333333,\n",
              " '[29]\\n\\nDomain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments.': 2.4761904761904763,\n",
              " '[2]\\n\\nThere are two general approaches to automatic summarization: extraction and abstraction.': 1.9047619047619049,\n",
              " '[30]\\n\\nThe main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.': 2.428571428571429,\n",
              " '[31]\\n• None Roxana, Angheluta (2002).': 0.26190476190476186,\n",
              " '[3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).': 1.464285714285714,\n",
              " '[4]\\n\\nThis has been applied mainly for text.': 0.7976190476190477,\n",
              " '[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system.': 1.2738095238095237,\n",
              " '[6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases.': 1.7023809523809523,\n",
              " '[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.': 2.511904761904762,\n",
              " '[citation needed]\\n\\nSubmodular Functions have also been used for other summarization tasks.': 1.7976190476190477,\n",
              " '[dubious – discuss]\\n\\nMulti-document extractive summarization faces a problem of potential redundancy.': 1.4166666666666667,\n",
              " 'doi:10.1007/978-3-319-66939-7_19.': 0.14285714285714285,\n",
              " 'doi:10.1007/978-3-642-35786-2_18.': 0.14285714285714285,\n",
              " \"• , Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\\n• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007).\": 0.33333333333333337,\n",
              " '• None Alrehamy, Hassan (2017).': 0.23809523809523808,\n",
              " '• None Elena, Lloret and Manuel, Palomar (2009).': 0.23809523809523808,\n",
              " '• None Marcu, Daniel (2000).': 0.23809523809523808}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOJdEavGAo0-"
      },
      "source": [
        "def visualize(title, sentence_list, best_sentences):\n",
        "  from IPython.core.display import HTML\n",
        "  text = ''\n",
        "\n",
        "  display(HTML(f'<h1>Summary - {title}</h1>'))\n",
        "  for sentence in sentence_list:\n",
        "    if sentence in best_sentences:\n",
        "      text += ' ' + str(sentence).replace(sentence, f\"<mark>{sentence}</mark>\")\n",
        "    else:\n",
        "      text += ' ' + sentence\n",
        "  display(HTML(f\"\"\" {text} \"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bp1x3GJmBIUZ",
        "outputId": "742c9c28-a982-4e75-a7de-f1c330f59e95"
      },
      "source": [
        "visualize(article.title, sentence_list, best_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h1>Summary - Automatic summarization - Wikipedia</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "  <mark>Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.</mark> In addition to text, images and videos can also be summarized. <mark>Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.</mark> <mark>[2]\n",
              "\n",
              "There are two general approaches to automatic summarization: extraction and abstraction.</mark> Here, content is extracted from the original data, but the extracted content is not modified in any way. <mark>Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.</mark> <mark>For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.</mark> [3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome). [4]\n",
              "\n",
              "This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. <mark>Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.</mark> <mark>\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.</mark> Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate. <mark>There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.</mark> <mark>The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).</mark> <mark>The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.</mark> Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs. <mark>An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.</mark> <mark>Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).</mark> This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary. <mark>Image collection summarization is another application example of automatic summarization.</mark> It consists in selecting a representative set of images from a larger set of images. [5] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured. <mark>At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.</mark> This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. <mark>Query based summarization techniques, additionally model for relevance of the summary with the query.</mark> <mark>Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.</mark> The task is the following. <mark>You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.</mark> [6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below. <mark>Consider the example text from a news article:\n",
              "\n",
              "A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases.</mark> These are pulled directly from the text. <mark>In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".</mark> Abstraction requires a deep understanding of the text, which makes it difficult for a computer system. Keyphrases have many applications. <mark>They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.</mark> Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme. Beginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem. <mark>Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below).</mark> We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases. After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall. Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization. Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision. <mark>We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.</mark> <mark>Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.</mark> The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper. In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number. Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. <mark>In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.</mark> The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases. Another keyphrase extraction algorithm is TextRank. <mark>While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.</mark> Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate. Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. <mark>Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.</mark> <mark>Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks.</mark> <mark>In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.</mark> Thus the algorithm is easily portable to new domains and languages. TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. <mark>For keyphrase extraction, it builds a graph using some set of text units as vertices.</mark> Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. <mark>Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).</mark> The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. <mark>For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.</mark> Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step. Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. <mark>Thus, \"natural\" and \"language\" might be linked in a text about NLP.</mark> <mark>\"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words.</mark> <mark>These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.</mark> Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text. It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. <mark>For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences.</mark> <mark>Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words.</mark> <mark>Running PageRank/TextRank on the graph is likely to rank \"learning\" highly.</mark> <mark>Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\".</mark> <mark>If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\".</mark> <mark>If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\".</mark> <mark>In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".</mark> In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. <mark>This approach has also been used in document summarization, considered below.</mark> <mark>Like keyphrase extraction, document summarization aims to identify the essence of a text.</mark> The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases. <mark>Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.</mark> The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references. It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary. If there are multiple references, the ROUGE-1 scores are averaged. <mark>Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.</mark> High-order n-gram ROUGE measures try to judge fluency to some degree. Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy. <mark>A promising line in document summarization is adaptive document/text summarization.</mark> <mark>[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.</mark> First summarizes that perform adaptive summarization have been created. <mark>[10]\n",
              "\n",
              "Supervised text summarization is very much like supervised keyphrase extraction.</mark> <mark>Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.</mark> <mark>Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.</mark> <mark>The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".</mark> This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. <mark>The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.</mark> Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams. <mark>During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.</mark> The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. <mark>Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies.</mark> <mark>Maximum entropy has also been applied successfully for summarization in the broadcast news domain.</mark> <mark>The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.</mark> <mark>Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.</mark> Then the sentences can be ranked with regard to their similarity to this centroid sentence. A more principled way to estimate sentence importance is using random walks and eigenvector centrality. <mark>LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.</mark> <mark>The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.</mark> In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document. The edges between sentences are based on some form of semantic similarity or content overlap. <mark>While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).</mark> The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights. In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. <mark>A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.</mark> <mark>It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.</mark> In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary. <mark>Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization.</mark> The task remains the same in both cases—only the number of sentences to choose from has grown. However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary. Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary. <mark>To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.</mark> The method used is called Cross-Sentence Information Subsumption (CSIS). <mark>These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader.</mark> Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. <mark>The importance of this sentence also stems from the importance of the sentences \"recommending\" it.</mark> <mark>Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.</mark> This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain. <mark>Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.</mark> Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. <mark>In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.</mark> Multi-document summarization may also be done in response to a question. [12] [4]\n",
              "\n",
              "Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. <mark>While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.</mark> Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. [dubious – discuss]\n",
              "\n",
              "Multi-document extractive summarization faces a problem of potential redundancy. <mark>Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another).</mark> LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results. <mark>There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.</mark> <mark>(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.)</mark> The algorithm is called GRASSHOPPER. <mark>[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).</mark> <mark>The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions.</mark> These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. [15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04. <mark>[16]\n",
              "\n",
              "A new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.</mark> <mark>This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).</mark> <mark>The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.</mark> Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. <mark>For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.</mark> The set cover function attempts to find a subset of objects which cover a given set of concepts. <mark>For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.</mark> This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. <mark>Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.</mark> <mark>While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization.</mark> For example, a simple greedy algorithm admits a constant factor guarantee. [17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems. Submodular functions have achieved state-of-the-art for almost all summarization problems. <mark>For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.</mark> <mark>Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions.</mark> <mark>This was a breakthrough result establishing submodular functions as the right models for summarization problems.</mark> <mark>[citation needed]\n",
              "\n",
              "Submodular Functions have also been used for other summarization tasks.</mark> <mark>Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization.</mark> Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets. <mark>[22]\n",
              "• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts.</mark> It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times. [24] The name is reference to TL;DR − Internet slang for \"too long; didn't read\". [25][26]\n",
              "\n",
              "The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries. Evaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual. <mark>[28]\n",
              "\n",
              "An intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.</mark> Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc. <mark>Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.</mark> <mark>Human judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult.</mark> Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage. <mark>One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]).</mark> It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. Anaphor resolution remains another problem yet to be fully solved. <mark>Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.</mark> <mark>[29]\n",
              "\n",
              "Domain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments.</mark> <mark>Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.</mark> <mark>For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.</mark> <mark>[30]\n",
              "\n",
              "The main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.</mark> This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). <mark>In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries.</mark> Moreover, they all perform a quantitative evaluation with regard to different similarity metrics. The first publication in the area dates back to 1958 (Lun), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. <mark>Pattern-based summarization was the most powerful option for multi-document summarization found by 2016.</mark> In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). <mark>Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.</mark> By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization. [31]\n",
              "• None Roxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization. • None Elena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation. • None Alrehamy, Hassan (2017). \"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\". Automatic Keyphrases Extraction. Advances in Intelligent Systems and Computing. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN . • None Marcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. ISBN . • , Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\n",
              "• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks . , The GRASSHOPPER algorithm\n",
              "• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). \"Summarizing Conceptual Graphs for Automatic Summarization Task\". Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN . , Conceptual Structures for STEM Research and Education. "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FuLiqlkqP-V"
      },
      "source": [
        "# Summarizing multiple texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ClgHXjCub2"
      },
      "source": [
        "article_list = ['https://en.wikipedia.org/wiki/Automatic_summarization',\n",
        "                'https://en.wikipedia.org/wiki/Natural_language_processing',\n",
        "                'https://en.wikipedia.org/wiki/Lemmatisation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VBr2mXahC6Gq",
        "outputId": "54d6b465-ba3c-4ca1-a5d4-67f4983a9cdc"
      },
      "source": [
        "for url in article_list:\n",
        "  #print(article)\n",
        "  g = Goose()\n",
        "  article = g.extract(url)\n",
        "  sentence_list, best_sentences, _, _ = summarize(article.cleaned_text, 100, percentage=0.5)\n",
        "  #print(len(sentence_list), len(best_sentences))\n",
        "  visualize(article.title, sentence_list, best_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h1>Summary - Automatic summarization - Wikipedia</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "  <mark>Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.</mark> In addition to text, images and videos can also be summarized. <mark>Text summarization finds the most informative sentences in a document;[1] image summarization finds the most representative images within an image collection[citation needed]; video summarization extracts the most important frames from the video content.</mark> <mark>[2]\n",
              "\n",
              "There are two general approaches to automatic summarization: extraction and abstraction.</mark> Here, content is extracted from the original data, but the extracted content is not modified in any way. <mark>Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.</mark> <mark>For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.</mark> <mark>[3] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).</mark> [4]\n",
              "\n",
              "This has been applied mainly for text. <mark>Abstractive methods build an internal semantic representation of the original content, and then use this representation to create a summary that is closer to what a human might express.</mark> <mark>Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction.</mark> <mark>Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.</mark> <mark>\"Paraphrasing\" is even more difficult to apply to image and video, which is why most summarization systems are extractive.</mark> <mark>Approaches aimed at higher summarization quality rely on combined software and human effort.</mark> In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate. <mark>There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on.</mark> <mark>The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).</mark> <mark>The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query.</mark> <mark>Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.</mark> <mark>An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.</mark> <mark>Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic).</mark> <mark>This problem is called multi-document summarization.</mark> A related application is summarizing news articles. <mark>Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.</mark> <mark>Image collection summarization is another application example of automatic summarization.</mark> It consists in selecting a representative set of images from a larger set of images. <mark>[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system.</mark> <mark>Video summarization is a related domain, where the system automatically creates a trailer of a long video.</mark> This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured. <mark>At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.</mark> This is also called the core-set. <mark>These algorithms model notions like diversity, coverage, information and representativeness of the summary.</mark> <mark>Query based summarization techniques, additionally model for relevance of the summary with the query.</mark> <mark>Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.</mark> The task is the following. <mark>You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.</mark> <mark>[6] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases.</mark> <mark>For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.</mark> <mark>Consider the example text from a news article:\n",
              "\n",
              "A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases.</mark> These are pulled directly from the text. <mark>In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\".</mark> Abstraction requires a deep understanding of the text, which makes it difficult for a computer system. Keyphrases have many applications. <mark>They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.</mark> Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme. <mark>Beginning with the work of Turney,[7] many researchers have approached keyphrase extraction as a supervised machine learning problem.</mark> <mark>Given a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below).</mark> We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. <mark>For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.</mark> After training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases. Keyphrase extractors are generally evaluated using precision and recall. Precision measures how many of the proposed keyphrases are actually correct. Recall measures how many of the true keyphrases your system proposed. The two measures can be combined in an F-score, which is the harmonic mean of the two (F = 2PR/(P + R) ). <mark>Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.</mark> Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. <mark>For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.</mark> Thus, recall may suffer. However, generating too many examples can also lead to low precision. <mark>We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.</mark> <mark>Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various boolean syntactic features (e.g., contains all caps), etc.</mark> The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney’s seminal paper. <mark>In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number.</mark> <mark>Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases.</mark> This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number. <mark>Once examples and features are created, we need a way to learn to predict keyphrases.</mark> Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. <mark>In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm.</mark> The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases. Another keyphrase extraction algorithm is TextRank. <mark>While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.</mark> Many documents with known keyphrases are needed. <mark>Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.</mark> Unsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. <mark>Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[8] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.</mark> <mark>Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks.</mark> <mark>In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties.</mark> Thus the algorithm is easily portable to new domains and languages. TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. <mark>For keyphrase extraction, it builds a graph using some set of text units as vertices.</mark> Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. <mark>Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).</mark> The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. <mark>However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases.</mark> This has a nice side effect of allowing us to produce keyphrases of arbitrary length. <mark>For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.</mark> Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step. Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2–10. <mark>Thus, \"natural\" and \"language\" might be linked in a text about NLP.</mark> <mark>\"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words.</mark> <mark>These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.</mark> <mark>Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases.</mark> The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. <mark>As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.</mark> It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. <mark>For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences.</mark> <mark>Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words.</mark> <mark>Running PageRank/TextRank on the graph is likely to rank \"learning\" highly.</mark> <mark>Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\".</mark> <mark>If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\".</mark> <mark>If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\".</mark> <mark>In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".</mark> In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. <mark>This approach has also been used in document summarization, considered below.</mark> <mark>Like keyphrase extraction, document summarization aims to identify the essence of a text.</mark> <mark>The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases.</mark> <mark>Before getting into the details of some summarization methods, we will mention how summarization systems are typically evaluated.</mark> The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure. <mark>This is a recall-based measure that determines how well a system-generated summary covers the content present in one or more human-generated model summaries known as references.</mark> It is recall-based to encourage systems to include all the important topics in the text. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. <mark>For example, ROUGE-1 is computed as division of count of unigrams in reference that appear in system and count of unigrams in reference summary.</mark> If there are multiple references, the ROUGE-1 scores are averaged. <mark>Because ROUGE is based only on content overlap, it can determine if the same general concepts are discussed between an automatic summary and a reference summary, but it cannot determine if the result is coherent or the sentences flow together in a sensible manner.</mark> High-order n-gram ROUGE measures try to judge fluency to some degree. Note that ROUGE is similar to the BLEU measure for machine translation, but BLEU is precision- based, because translation systems favor accuracy. <mark>A promising line in document summarization is adaptive document/text summarization.</mark> <mark>[9] The idea of adaptive summarization involves preliminary recognition of document/text genre and subsequent application of summarization algorithms optimized for this genre.</mark> <mark>First summarizes that perform adaptive summarization have been created.</mark> <mark>[10]\n",
              "\n",
              "Supervised text summarization is very much like supervised keyphrase extraction.</mark> <mark>Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.</mark> <mark>Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.</mark> <mark>The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".</mark> This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. <mark>The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.</mark> Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 only cares about unigrams. <mark>During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.</mark> The system was based on a hybrid system using a naive Bayes classifier and statistical language models for modeling salience. <mark>Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies.</mark> <mark>Maximum entropy has also been applied successfully for summarization in the broadcast news domain.</mark> <mark>The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.</mark> <mark>Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.</mark> Then the sentences can be ranked with regard to their similarity to this centroid sentence. A more principled way to estimate sentence importance is using random walks and eigenvector centrality. <mark>LexRank[11] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.</mark> <mark>The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.</mark> In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document. The edges between sentences are based on some form of semantic similarity or content overlap. <mark>While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths).</mark> <mark>The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score.</mark> TextRank uses continuous similarity scores as weights. In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. <mark>A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.</mark> <mark>It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.</mark> In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary. <mark>Another important distinction is that TextRank was used for single document summarization, while LexRank has been applied to multi-document summarization.</mark> The task remains the same in both cases—only the number of sentences to choose from has grown. <mark>However, when summarizing multiple documents, there is a greater risk of selecting duplicate or highly redundant sentences to place in the same summary.</mark> Imagine you have a cluster of news articles on a particular event, and you want to produce one summary. <mark>Each article is likely to have many similar sentences, and you would only want to include distinct ideas in the summary.</mark> <mark>To address this issue, LexRank applies a heuristic post-processing step that builds up a summary by adding sentences in rank order, but discards any sentences that are too similar to ones already placed in the summary.</mark> The method used is called Cross-Sentence Information Subsumption (CSIS). <mark>These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader.</mark> Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. <mark>The importance of this sentence also stems from the importance of the sentences \"recommending\" it.</mark> <mark>Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences.</mark> This makes intuitive sense and allows the algorithms to be applied to any arbitrary new text. The methods are domain-independent and easily portable. <mark>One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain.</mark> <mark>However, the unsupervised \"recommendation\"-based approach applies to any domain.</mark> <mark>Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.</mark> <mark>Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents.</mark> <mark>In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.</mark> <mark>Multi-document summarization may also be done in response to a question.</mark> <mark>[12] [4]\n",
              "\n",
              "Multi-document summarization creates information reports that are both concise and comprehensive.</mark> With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. <mark>While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.</mark> Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. <mark>[dubious – discuss]\n",
              "\n",
              "Multi-document extractive summarization faces a problem of potential redundancy.</mark> <mark>Ideally, we would like to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another).</mark> <mark>LexRank deals with diversity as a heuristic final stage using CSIS, and other systems have used similar methods, such as Maximal Marginal Relevance (MMR),[13] in trying to eliminate redundancy in information retrieval results.</mark> <mark>There is a general purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks.</mark> <mark>(An absorbing random walk is like a standard random walk, except some states are now absorbing states that act as \"black holes\" that cause the walk to end abruptly at that state.)</mark> The algorithm is called GRASSHOPPER. <mark>[14] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).</mark> <mark>The state of the art results for multi-document summarization, however, are obtained using mixtures of submodular functions.</mark> These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. <mark>[15] Similar results were also achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.</mark> <mark>[16]\n",
              "\n",
              "A new method for multi-lingual multi-document summarization that avoids redundancy works by simplifying and generating ideograms that represent the meaning of each sentence in each document and then evaluates similarity \"qualitatively\" by comparing the shape and position of said ideograms has recently been developed.</mark> <mark>This tool does not use word frequency, does not need training or preprocessing of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary).</mark> <mark>The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.</mark> Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. <mark>For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular.</mark> The set cover function attempts to find a subset of objects which cover a given set of concepts. <mark>For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.</mark> This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. <mark>Another example of a submodular optimization problem is using a determinantal point process to model diversity.</mark> Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined together, and the resulting function is still submodular. <mark>Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.</mark> <mark>While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization.</mark> For example, a simple greedy algorithm admits a constant factor guarantee. <mark>[17] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.</mark> <mark>Submodular functions have achieved state-of-the-art for almost all summarization problems.</mark> <mark>For example, work by Lin and Bilmes, 2012[18] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.</mark> <mark>Similarly, work by Lin and Bilmes, 2011,[19] shows that many existing systems for automatic summarization are instances of submodular functions.</mark> <mark>This was a breakthrough result establishing submodular functions as the right models for summarization problems.</mark> <mark>[citation needed]\n",
              "\n",
              "Submodular Functions have also been used for other summarization tasks.</mark> <mark>Tschiatschek et al., 2014 show[20] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization.</mark> Similarly, Bairi et al., 2015[21] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets. <mark>[22]\n",
              "• The Reddit bot \"autotldr\",[23] created in 2011 summarizes news articles in the comment-section of reddit posts.</mark> It was found to be very useful by the reddit community which upvoted its summaries hundreds of thousands of times. <mark>[24] The name is reference to TL;DR − Internet slang for \"too long; didn't read\".</mark> <mark>[25][26]\n",
              "\n",
              "The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.</mark> Evaluation techniques fall into intrinsic and extrinsic,[27] inter-textual and intra-textual. <mark>[28]\n",
              "\n",
              "An intrinsic evaluation tests the summarization system in and of itself while an extrinsic evaluation tests the summarization based on how it affects the completion of some other task.</mark> Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. <mark>Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.</mark> <mark>Intra-textual methods assess the output of a specific summarization system, and the inter-textual ones focus on contrastive analysis of outputs of several summarization systems.</mark> <mark>Human judgement often has wide variance on what is considered a \"good\" summary, which means that making the evaluation process automatic is particularly difficult.</mark> <mark>Manual evaluation can be used, but this is both time and labor-intensive as it requires humans to read not only the summaries but also the source documents.</mark> Other issues are those concerning coherence and coverage. <mark>One of the metrics used in NIST's annual Document Understanding Conferences, in which research groups submit their systems for both summarization and translation tasks, is the ROUGE metric (Recall-Oriented Understudy for Gisting Evaluation [2]).</mark> It essentially calculates n-gram overlaps between automatically generated summaries and previously-written human summaries. A high level of overlap should indicate a high level of shared concepts between the two summaries. Note that overlap metrics like this are unable to provide any feedback on a summary's coherence. Anaphor resolution remains another problem yet to be fully solved. <mark>Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.</mark> <mark>[29]\n",
              "\n",
              "Domain independent summarization techniques generally apply sets of general features which can be used to identify information-rich text segments.</mark> <mark>Recent research focus has drifted to domain-specific summarization techniques that utilize the available knowledge specific to the domain of text.</mark> <mark>For example, automatic summarization research on medical text generally attempts to utilize the various sources of codified medical knowledge and ontologies.</mark> <mark>[30]\n",
              "\n",
              "The main drawback of the evaluation systems existing so far is that we need at least one reference summary, and for some methods more than one, to be able to compare automatic summaries with models.</mark> This is a hard and expensive task. Much effort has to be done in order to have corpus of texts and their corresponding summaries. Furthermore, for some methods, not only do we need to have human-made summaries available for comparison, but also manual annotation has to be performed in some of them (e.g. SCU in the Pyramid Method). <mark>In any case, what the evaluation methods need as an input, is a set of summaries to serve as gold standards and a set of automatic summaries.</mark> Moreover, they all perform a quantitative evaluation with regard to different similarity metrics. The first publication in the area dates back to 1958 (Lun), starting with a statistical technique. Research increased significantly in 2015. Term frequency–inverse document frequency had been used by 2016. <mark>Pattern-based summarization was the most powerful option for multi-document summarization found by 2016.</mark> In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). <mark>Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.</mark> <mark>By 2020 the field was still very active and research is shifting towards abstractive summation and real-time summarization.</mark> [31]\n",
              "• None Roxana, Angheluta (2002). The Use of Topic Segmentation for Automatic Summarization. • None Elena, Lloret and Manuel, Palomar (2009). Challenging Issues of Automatic Summarization: Relevance Detection and Quality-based Evaluation. • None Alrehamy, Hassan (2017). <mark>\"SemCluster: Unsupervised Automatic Keyphrase Extraction Using Affinity Propagation\".</mark> Automatic Keyphrases Extraction. Advances in Intelligent Systems and Computing. 650. pp. 222–235. doi:10.1007/978-3-319-66939-7_19. ISBN . • None Marcu, Daniel (2000). The Theory and Practice of Discourse Parsing and Summarization. ISBN . • , Published in Proceeding RIAO'10 Adaptivity, Personalization and Fusion of Heterogeneous Information, CID Paris, France\n",
              "• None Xiaojin, Zhu, Andrew Goldberg, Jurgen Van Gael, and David Andrzejewski (2007). Improving diversity in ranking using absorbing random walks . , The GRASSHOPPER algorithm\n",
              "• None Miranda-Jiménez, Sabino, Gelbukh, Alexander, and Sidorov, Grigori (2013). <mark>\"Summarizing Conceptual Graphs for Automatic Summarization Task\".</mark> Conceptual Structures for STEM Research and Education. Lecture Notes in Computer Science. 7735. pp. 245–253. doi:10.1007/978-3-642-35786-2_18. ISBN . , Conceptual Structures for STEM Research and Education. "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h1>Summary - Natural language processing - Wikipedia</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "  <mark>Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.</mark> The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. Natural language processing has its roots in the 1950s. <mark>Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.</mark> <mark>The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.</mark> • 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. [2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. <mark>Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.</mark> <mark>• 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.</mark> Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. <mark>When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".</mark> <mark>• 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.</mark> Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first many chatterbots were written (e.g., PARRY). • 1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. <mark>Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[3]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[4]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory).</mark> Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. <mark>[5]\n",
              "\n",
              "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.</mark> <mark>Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.</mark> This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. <mark>[6]\n",
              "• 1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research.</mark> <mark>These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.</mark> <mark>However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.</mark> As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data. <mark>• 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s.</mark> Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. <mark>Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.</mark> However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical. <mark>In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others.</mark> This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care. <mark>[12]\n",
              "\n",
              "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.</mark> <mark>More recent systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
              "• The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.</mark> <mark>• Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g.</mark> containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). <mark>Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.</mark> <mark>• Systems based on automatically learning the rules can be made more accurate simply by supplying more input data.</mark> <mark>However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task.</mark> <mark>In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable.</mark> <mark>However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.</mark> <mark>Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used\n",
              "• when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,\n",
              "• for preprocessing in NLP pipelines, e.g., tokenization, or\n",
              "• for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.</mark> <mark>Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning.</mark> <mark>The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.</mark> Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. <mark>These algorithms take as input a large set of \"features\" that are generated from the input data.</mark> <mark>Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.</mark> Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system. <mark>Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.</mark> <mark>However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.</mark> <mark>The cache language models upon which many speech recognition systems now rely are examples of such statistical models.</mark> <mark>Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.</mark> Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required. A major drawback of statistical methods is that they require elaborate feature engineering. <mark>Since 2015,[17] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning.</mark> <mark>Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).</mark> <mark>In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.</mark> <mark>For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).</mark> Latest works tend to use non-technical structure of a given task to build proper neural network. <mark>[18]\n",
              "\n",
              "The following is a list of some of the most commonly researched tasks in natural language processing.</mark> Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks. Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below. Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. <mark>As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[35]\n",
              "• Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\n",
              "• Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n",
              "\n",
              "Most more higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language.</mark> More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses. \"[36] Cognitive science is the interdisciplinary, scientific study of the mind and its processes. [37] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. [38] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. <mark>As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[39] with two defining aspects:\n",
              "• Apply the theory of conceptual metaphor, explained by Lakoff as “the understanding of one idea, in terms of another” which provides an idea of the intent of the author.</mark> [40] For example, consider the English word “big”. <mark>When used in a comparison (“That is a big tree”), the author's intent is to imply that the tree is ”physically large” relative to other trees or the authors experience.</mark> When used metaphorically (”Tomorrow is a big day”), the author’s intent to imply ”importance”. The intent behind other usages, like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information. • Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). <mark>The mathematical equation for such algorithms is presented in :\n",
              "\n",
              "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s.</mark> <mark>Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[41] functional grammar,[42] construction grammar,[43] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[44] of the ACL).</mark> More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". [45] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit). [46] "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h1>Summary - Lemmatisation - Wikipedia</h1>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "  <mark>Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.</mark> <mark>[1]\n",
              "\n",
              "In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning.</mark> <mark>Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.</mark> As a result, developing efficient lemmatisation algorithms is an open area of research. [2][3][4]\n",
              "\n",
              "In many languages, words appear in several inflected forms. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks' or 'walking'. <mark>The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word.</mark> <mark>The association of the base form with a part of speech is often called a lexeme of the word.</mark> Lemmatisation is closely related to stemming. <mark>The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech.</mark> However, stemmers are typically easier to implement and run faster. The reduced \"accuracy\" may not matter for some applications. <mark>In fact, when used within information retrieval systems, stemming improves query recall accuracy, or true positive rate, when compared to lemmatisation.</mark> Nonetheless, stemming reduces precision, or the proportion of positively-labeled instances that are actually positive, for such systems. <mark>[5]\n",
              "• The word \"better\" has \"good\" as its lemma.</mark> This link is missed by stemming, as it requires a dictionary look-up. <mark>• The word \"walk\" is the base form for the word \"walking\", and hence this is matched in both stemming and lemmatisation.</mark> <mark>• The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context; e.g., \"in our last meeting\" or \"We are meeting again tomorrow\".</mark> <mark>Unlike stemming, lemmatisation attempts to select the correct lemma depending on the context.</mark> <mark>Document indexing software like Lucene[6] can store the base stemmed format of the word without the knowledge of meaning, but only considering word formation grammar rules.</mark> <mark>The stemmed word itself might not be a valid word: 'lazy', as seen in the example below, is stemmed by many stemmers to 'lazi'.</mark> This is because the purpose of stemming is not to produce the appropriate lemma – that is a more challenging task that requires knowledge of context. <mark>The main purpose of stemming is to map different forms of a word to a single form.</mark> <mark>[7] As a rule-based algorithm, dependent only upon the spelling of a word, it sacrifices accuracy to ensure that, for example, when 'laziness' is stemmed to 'lazi', it has the same stem as 'lazy'.</mark> A trivial way to do lemmatization is by simple dictionary lookup. This works well for straightforward inflected forms, but a rule-based system will be needed for other cases, such as in languages with long compound words. Such rules can be either hand-crafted or learned automatically from an annotated corpus. Morphological analysis of published biomedical literature can yield useful results. Morphological processing of biomedical text can be more effective by a specialised lemmatisation program for biomedicine, and may improve the accuracy of practical information extraction tasks. [8] "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}